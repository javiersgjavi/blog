---
title: (DRAFT) Large Language Masked Diffusion Models (LLaDA)
date: 2025-09-16 00:02:30 +/-1200
categories: [Explanation]
tags: [diffusion, llm]     ## TAG names should always be lowercase
math: true
image:
    path: assets/posts/2025-09-16-llada/portada.png
    alt: Text generated by a Masked Diffusion Model
---

# LLaDA: Diffusion Models que Finalmente Funcionan para Texto

## Introducci√≥n Personal y Motivaci√≥n

A principios de este a√±o estuve trabajando intensamente con modelos de difusi√≥n y modelos de consistencia aplicados a la generaci√≥n de embeddings en espacios latentes. Mi objetivo era ambicioso: conseguir que estos enfoques funcionaran para texto de manera similar a como revolucionaron la generaci√≥n de im√°genes.

Me inspir√© principalmente en tres papers fundamentales que exploraban diferentes aproximaciones al problema:

1. **"Latent Diffusion for Language Generation"** de Justin Lovelace
2. **"TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings"**
3. **"Continuous diffusion for categorical data"**

Sin embargo, a pesar de seguir estas investigaciones y probar m√∫ltiples combinaciones y variaciones propias, los resultados que obtuve no ten√≠an la calidad que esperaba y presentaban una complejidad considerable tanto en implementaci√≥n como en entrenamiento.

{: .prompt-warning }
> **Advertencia sobre la complejidad**: Los modelos de difusi√≥n para texto han sido hist√≥ricamente mucho m√°s desafiantes que para im√°genes. Mi experiencia personal lo confirma: meses de trabajo intenso con resultados frustrantes.

Despu√©s de varios meses de experimentaci√≥n, logr√© desarrollar un modelo de consistencia que funcionaba parcialmente, pero no al nivel que deseaba. Los modelos de difusi√≥n tradicionales que implement√© simplemente no llegaban a generar texto coherente - algo que me frustraba enormemente dado el √©xito que estos modelos hab√≠an tenido en otros dominios.

Fue entonces cuando me top√© con el paper de LLaDA (Large Language Diffusion Models with Masking). Lo que inmediatamente me llam√≥ la atenci√≥n fue su claridad conceptual, su intuitividad y, sobre todo, su enfoque directo al problema. A diferencia de mis intentos anteriores que se perd√≠an en complejidades t√©cnicas, LLaDA presentaba una soluci√≥n elegante y comprensible.

Es importante mencionar que LLaDA no aborda espec√≠ficamente la generaci√≥n de embeddings - el problema concreto en el que yo estaba trabajando. Sin embargo, me di cuenta de que mi enfoque de trabajar con embeddings era en realidad una forma de simplificar la tarea, evitando lidiar directamente con la naturaleza discreta del texto. El paper de LLaDA me mostr√≥ que era posible abordar el problema de manera directa, trabajando con tokens discretos mediante enmascaramiento, y que este enfoque podr√≠a ser perfectamente aplicable a mi tarea original. De ah√≠ mi inmediato inter√©s.

La diferencia fue notable: mientras que mis implementaciones previas requer√≠an semanas de debugging y ajuste de hiperpar√°metros, pude hacer funcionar LLaDA con relativamente poco trabajo de implementaci√≥n. Esto no solo valid√≥ el enfoque, sino que me hizo reflexionar sobre la importancia de la simplicidad conceptual en el dise√±o de modelos.

En este post voy a compartir todo lo que he aprendido sobre LLaDA, un modelo que desaf√≠a uno de los paradigmas m√°s establecidos en NLP: la generaci√≥n autoregresiva. M√°s importante a√∫n, voy a explicar por qu√© este enfoque funciona donde otros (incluidos los m√≠os) hab√≠an fallado.

{: .prompt-info }
Nota breve y especulativa: sospecho que el enfoque de LLaDA ‚Äîdifusi√≥n discreta con enmascarado y predicci√≥n paralela de tokens‚Äî est√° relacionado con lo que Google DeepMind denomina "Gemini Diffusion". La descripci√≥n p√∫blica menciona generaci√≥n por bloques e iteraci√≥n con refinamiento, se√±ales que encajan con esta familia de m√©todos. A√∫n as√≠, es solo una hip√≥tesis hasta que publiquen el paper t√©cnico. Referencia: [deepmind.google/models/gemini-diffusion](https://deepmind.google/models/gemini-diffusion/).

### Problemas que resuelve LLaDA

#### Generaci√≥n Autoregresiva vs Difusi√≥n

Una de las contribuciones m√°s importantes del paper de LLaDA es cuestionar una suposici√≥n fundamental que la comunidad de NLP hab√≠a aceptado casi sin debate: **¬øes la generaci√≥n autoregresiva realmente necesaria para crear modelos de lenguaje inteligentes?**

Los autores argumentan algo que, en retrospectiva, parece obvio pero que pocos hab√≠an verbalizado claramente: la habilidad de generar texto coherente no es espec√≠fica de los modelos autoregresivos. Esta capacidad se debe m√°s bien a tres factores fundamentales:

1. **La arquitectura Transformer** y su capacidad de modelar dependencias complejas
2. **La cantidad masiva de datos** utilizados durante el entrenamiento
3. **El proceso de entrenamiento generativo** que ense√±a al modelo a capturar patrones estad√≠sticos

Por lo tanto, capacidades emergentes como el aprendizaje en contexto (in-context learning) y la habilidad de seguir instrucciones tampoco deber√≠an ser exclusivas de los modelos autoregresivos.

{: .prompt-info }
> **Reflexi√≥n personal**: Cuando le√≠ esta argumentaci√≥n por primera vez, me pareci√≥ una de esas ideas que son "obviamente correctas" una vez que alguien las explicita, pero que nadie hab√≠a cuestionado seriamente antes.

##### Los Problemas Inherentes de la Generaci√≥n Autoregresiva

**Costo Computacional Prohibitivo**

El primer problema que LLaDA aborda es la ineficiencia fundamental de la generaci√≥n autoregresiva. Cada vez que queremos generar un nuevo token, debemos:

1. Volver a computar la representaci√≥n de toda la secuencia anterior
2. Realizar un paso completo por la red neuronal
3. Repetir este proceso para cada token individual

Esto significa que para generar una secuencia de 100 tokens, necesitamos realizar 100 pasadas por la red. Es como si cada vez que quisi√©ramos a√±adir una palabra a un p√°rrafo, tuvi√©ramos que releer todo el texto desde el principio.

**Limitaciones Direccionales del Razonamiento**

El segundo problema es m√°s sutil pero igualmente importante: la generaci√≥n autoregresiva solo puede procesar texto de izquierda a derecha. Esta restricci√≥n direccional limita la capacidad de razonamiento en tareas espec√≠ficas donde el contexto bidireccional es crucial.

Un ejemplo perfecto son las tareas de "reversi√≥n" que menciona el paper: si le pides a un modelo autoregresivo que complete un poema hacia atr√°s, tendr√° enormes dificultades porque nunca ha aprendido a razonar en esa direcci√≥n. Es como si hubiera aprendido a leer pero nunca a escribir de derecha a izquierda.

**Eficiencia de Aprendizaje Limitada**

Finalmente, y esto es algo que me parece particularmente interesante, los autores argumentan que los modelos de difusi√≥n tienen una **mayor capacidad para extraer aprendizaje de la misma cantidad de datos**. Esto se debe a que pueden ver el contexto completo (bidireccional) durante el entrenamiento, lo que les permite:

- Capturar patrones m√°s complejos en los datos
- Ser m√°s eficientes durante el entrenamiento
- Escalar mejor con la misma cantidad de recursos

{: .prompt-warning }
> **Importante**: Esta √∫ltima afirmaci√≥n sobre eficiencia de aprendizaje es una de las m√°s controvertidas del paper. Aunque los resultados experimentales la respaldan, a√∫n necesitamos m√°s investigaci√≥n para confirmarla completamente.

En mi experiencia implementando tanto modelos autoregresivos como LLaDA, la diferencia en la velocidad de convergencia durante el entrenamiento fue notable. LLaDA parec√≠a "entender" los patrones en los datos m√°s r√°pidamente, aunque esto podr√≠a deberse a m√∫ltiples factores.

#### Generaci√≥n con Difusi√≥n: Los Intentos Anteriores

Para entender por qu√© LLaDA representa un avance tan significativo, es crucial comprender los obst√°culos que enfrentaron los intentos previos de aplicar difusi√≥n al texto. La historia de estos experimentos est√° llena de frustraciones t√©cnicas que conozco de primera mano.

##### El Problema Fundamental: Discreto vs Continuo

Los modelos de difusi√≥n originales, como DDPM, fueron dise√±ados para trabajar con **espacios latentes continuos**. Esto funciona perfectamente para im√°genes, donde cada p√≠xel puede tomar cualquier valor en un rango continuo (por ejemplo, de 0 a 255 en RGB). Sin embargo, el texto es inherentemente **discreto**: cada token es una entidad espec√≠fica del vocabulario, no un punto en un espacio continuo.

Los primeros intentos de aplicar difusi√≥n al texto trataron de sortear este problema fundamental movi√©ndose del espacio discreto de tokens a espacios continuos de embeddings. La l√≥gica parec√≠a s√≥lida: si podemos representar cada token como un vector de embeddings, entonces podemos aplicar difusi√≥n gaussiana a estos vectores continuos (Diffusion-LM Improves Controllable Text Generation). 

Tambi√©n hubo algunos enfoques los cuales trataron de adaptar el proceso de poner ruido gausiano en un espacio continuo a uno similar en un espacio discreto. (Structured Denoising Diffusion Models in Discrete State-Spaces)

**Pero los resultados no fueron los esperados.**

##### Mi Experiencia Personal: El Problema de las Regiones Inv√°lidas

Durante mis meses experimentando con latent diffusion models para generar embeddings de texto, me top√© exactamente con el problema que se describe en el paper de "Reflected Diffusion Models": **el problema de las regiones inv√°lidas del espacio latente**.

{: .prompt-warning }
> **El problema de las regiones inv√°lidas**: Durante el proceso de eliminaci√≥n de ruido, los modelos pueden cometer errores que los llevan a regiones del espacio latente que no corresponden a ning√∫n token o secuencia v√°lida.

Cuando trabajas con p√≠xeles en im√°genes, este problema es manejable. Si el modelo predice un valor de p√≠xel ligeramente incorrecto, obtienes una imagen un poco borrosa o con artefactos, pero sigue siendo una imagen interpretable. Sin embargo, cuando trabajas con embeddings que representan secuencias de tokens, los errores en la eliminaci√≥n de ruido pueden llevarte a vectores que:

1. **No corresponden a ning√∫n token real** del vocabulario
2. **Representan combinaciones sem√°nticamente inconsistentes**
3. **Son imposibles de mapear de vuelta al espacio discreto** de manera coherente

En mi implementaci√≥n, esto se traduc√≠a en generaciones que parec√≠an "casi correctas" pero que al intentar convertirlas de vuelta a tokens discretos, produc√≠an secuencias sin sentido o completamente incoherentes.

##### El Escalamiento Limitado

Adem√°s de estos problemas t√©cnicos, los enfoques de difusi√≥n continua para texto mostraron **limitaciones severas de escalamiento**. Los modelos que implement√©, aunque funcionaban en ejemplos peque√±os, no lograban escalar a vocabularios grandes o secuencias largas de manera efectiva. La complejidad de mantener la coherencia sem√°ntica en el espacio continuo crec√≠a exponencialmente con el tama√±o del problema.

##### El Nacimiento de los Masked Diffusion Models

Fue en este contexto de frustraci√≥n con los enfoques continuos que nacieron los **Masked Diffusion Models**. Aunque t√©cnicamente no son exactamente iguales a los modelos de difusi√≥n tradicionales, comparten una idea fundamental similar:

**En lugar de partir de ruido gaussiano, partimos de tokens completamente enmascarados.**

La analog√≠a es elegante:
- **DDPM tradicional**: Ruido gaussiano ‚Üí Datos limpios  
- **Masked Diffusion**: Tokens enmascarados ‚Üí Texto real

El proceso de "eliminaci√≥n de ruido" se convierte en un proceso de **desenmascaramiento progresivo**, donde:

1. Comenzamos con una secuencia completamente enmascarada: `[MASK] [MASK] [MASK] [MASK]`
2. Gradualmente vamos revelando tokens: `[MASK] the [MASK] [MASK]`
3. Continuamos hasta obtener texto completo: `Hello the world today`

{: .prompt-info }
> **Insight clave**: Este enfoque evita completamente el problema de las regiones inv√°lidas porque siempre trabajamos directamente en el espacio discreto de tokens. No hay conversi√≥n problem√°tica entre espacios continuos y discretos.

Esta transici√≥n conceptual de "difusi√≥n gaussiana" a "difusi√≥n discreta con enmascaramiento" fue el breakthrough que permiti√≥ que modelos como LLaDA finalmente funcionaran donde otros hab√≠an fallado.

## ¬øC√≥mo Funciona LLaDA?

Ahora que hemos establecido por qu√© los enfoques anteriores fallaron y c√≥mo surgi√≥ la idea de los Masked Diffusion Models, es momento de sumergirnos en los detalles t√©cnicos de LLaDA. Lo que hace brillante a este enfoque es su simplicidad conceptual: toma la intuici√≥n exitosa de la difusi√≥n y la adapta elegantemente al mundo discreto del texto.

### Masked Diffusion: La Innovaci√≥n Central

LLaDA implementa lo que podr√≠amos llamar "difusi√≥n discreta" - un proceso que mantiene la estructura iterativa de los modelos de difusi√≥n tradicionales pero opera completamente en el espacio de tokens discretos.

La diferencia fundamental es elegante en su simplicidad:

- **DDPM tradicional**: \\( \text{Ruido gaussiano} \xrightarrow{\text{eliminaci√≥n de ruido}} \text{Datos limpios} \\)
- **LLaDA**: \\( \text{Tokens enmascarados} \xrightarrow{\text{desenmascaramiento}} \text{Texto real} \\)

#### Proceso Forward (Corrupci√≥n): 

En lugar de a√±adir ruido gaussiano, LLaDA gradualmente enmascara tokens con probabilidad creciente a lo largo de los pasos de difusi√≥n.

Matem√°ticamente, para una secuencia de datos \\(x_0 \sim p_{\text{data}}\\), el proceso de corrupci√≥n se define como:

$$
\begin{equation}
q(x_t|x_0) = \text{Mask}(x_0, t)
\label{eq:forward_mask}
\end{equation}
$$

Donde el proceso de enmascaramiento funciona de la siguiente manera:

1. **Muestreamos el nivel de corrupci√≥n**: \\(t \sim \mathcal{U}(0,1]\\)
2. **Generamos una m√°scara aleatoria**: \\(M \sim \mathcal{U}(0,1]^{|x_0|}\\) con la misma longitud que \\(x_0\\)
3. **Aplicamos el enmascaramiento**:

$$
\begin{equation}
x_t[i] = \begin{cases}
\text{[MASK]} & \text{si } M[i] < t \\
x_0[i] & \text{si } M[i] \geq t
\end{cases}
\label{eq:masking_rule}
\end{equation}
$$

Esta formulaci√≥n es an√°loga al proceso forward de DDPM, pero en lugar de a√±adir ruido gaussiano, aplicamos enmascaramiento estoc√°stico. El par√°metro \\(t\\) controla la intensidad de la corrupci√≥n: cuando \\(t \rightarrow 0\\), pocos tokens se enmascaran; cuando \\(t \rightarrow 1\\), la mayor√≠a de tokens se convierten en \\(\text{[MASK]}\\).

Durante el entrenamiento, lo que vamos a intentar optimizar es la siguiente funci√≥n de p√©rdidas:

$$
\begin{equation}
\mathcal{L}  = - \frac{1}{t*L} \sum^L_{i=1} {\mathbf{1}[x_t^i = M] \log p_\theta(x^i_0|x_t)} 
\label{eq:loss}
\end{equation}
$$

Qu√© b√°sicamente lo que hace es comparar las distribuciones predichas por el modelo de cada uno de los tokens de la secuencia, pero sol√°mente para aquellas posiciones que estaban inicialmente enmascaradas. Es importante comentar que hay un factor de suavizado en funci√≥n de t, ya que no se penaliza igual al modelo por un fallo cuando est√° la secuencia totalmente enmascarada, y por lo tanto, por definir ($$ t=1 $$), que cuando estamos ya refinando los √∫ltimos tokens ($$ t=0.05 $$)

![Generation process](assets/posts/2025-09-16-llada/predictor.png){: width="400" height="400" }

#### Proceso Reverse (Generaci√≥n): 

En lugar de predecir ruido para eliminarlo, el modelo predice directamente qu√© tokens deber√≠an ocupar las posiciones enmascaradas, y lo hace simult√°neamente para m√∫ltiples posiciones. Esto lo hacemos de manera iterativa y gradual, recorriendo los posibles valores de $$ t $$, que estaban en un rango de $$ t \in [0,1] $$ desde al m√°ximo al m√≠nimo en una cantidad de $$ T $$ pasos que debemos definir.

El proceso reverse se formaliza como:

$$
\begin{equation}
p_\theta(x_{t-1}|x_t) = \text{Predict}(x_t, t) \rightarrow \hat{x}_t
\hat{x}_0 = \text{arg max} p_\theta(x_0 | x_t)
\label{eq:reverse_predict}
\end{equation}
$$

El modelo \\(\theta\\) toma como entrada la secuencia parcialmente enmascarada \\(x_t\\), y produce predicciones simult√°neas de la distribuci√≥n de probabilidad de los tokens para todas las posiciones enmascaradas. Una posible forma de recuperar los tokens finalmente podr√≠a ser muestrear los tokens con mayor probabilidad de estas distribuciones para cada posici√≥n, tal y como se muestra en la Ecuaci√≥n \ref{eq:reverse_predict}, aunque se podr√≠an aplicar otros m√©todos, como un muestreo desde la distribuci√≥n. Crucialmente, esto permite **generaci√≥n paralela** de m√∫ltiples tokens, contrastando con la naturaleza secuencial de los modelos autoregresivos.

Una vez obtenidas las predicciones \\(\hat{x}_t\\), necesitamos aplicar una estrategia de **re-enmascaramiento** para decidir qu√© tokens mantener y cu√°les volver a enmascarar en el siguiente paso, ya que de la red vamos a obtener la distribuci√≥n de todos los tokens, pero el proceso de generaci√≥n debe de ser gradual e ir refinando la predicci√≥n poco a poco.

En cuanto a los m√©todos para ello, hay muchas t√©cnicas que se puede aplicar:

    - Ir seleccionando gradualmente de forma probabil√≠stica que tokens nos quedamos, de forma que si uno ya ha sido fijado se mantenga siempre as√≠ y cada vez la secuencia est√© m√°s descubierta.

    - El paper tambi√©n propone un m√©todo un poco m√°s inteligente que propone que se enmascara de forma probabil√≠stica, como el anterior, pero dando prioridad a aquellos tokens que tienen una distribuci√≥n m√°s uniforme, y por lo tanto, menor confianza en la predicci√≥n.

De nuevo, una de las cosas interesantes del modelo de diffusion versus un modelo autorregresivo es la eficiencia. Si bien para generar una secuencia de N tokens hacen falta N pasos por la red para un modelo autorregresivo, en un modelo de difusi√≥n solamente hacen falta T pasos por la red. Si bien es cierto que estos modelos se benefici√°n con mayores valores de T, para poder refinar la predicci√≥n, si llegamos a escalas en las que N >> T, empezamos a encontrar beneficios en la velocidad de generaci√≥n.

#### Otras consideraciones

Es importante destacar que si la explicaci√≥n anterior es la base de LLaDA, tambi√©n es posible extender el entrenamiento a una posible fase de Fine-Tuning en el cual el modelo tenga un prompt y enmascaramos solo la regi√≥n de la secuencia de respuesta destinada a la respuesta generada por el LLM, similar al Fine-tuning que se hace con modelos autorregresivos para que aprendan a aceptar prompts, el in-context learning o following instructions.

La extensi√≥n a estos m√©todos es bastante sencilla, simplemente teniendo que modelar el problema como uno de generaci√≥n condicional con un prompt de entrada $$ p_\theta (x_0 | p_0, x_t) $$, d√≥nde $$ p_0 $$ ser√≠a la secuencia referente al prompt. Sin embargo, en este post no vamos a cubrir tanto esa parte, ya que creo que las bases del m√©todo han quedado m√°s o menos claras y no quiero alargar a√∫n m√°s la extensi√≥n de este post.

![Generation process](assets/posts/2025-09-16-llada/prompt.png){: width="400" height="400" }

## Resultados de LLaDA

### Modelo utilizado

Lo primero que hay que comentar es que debido al tipo de generaci√≥n basada en m√°scaras y teniendo en cuenta toda la secuencia a la vez, en lugar de izquierda a derecha como los modelos autorregresivos, un modelo basado en LLaDA deber√≠a de implementarse con modelos basados en transformers no causales, d√≥nde estos puedan presetar atenci√≥n para cada token a cualquier otro de la secuencia. Por lo tanto, tampoco es posible aplicar KV caching.

En el paper, ellos implementan dos modelo transformer b√°sicos, similar a muchos otros LLMs, con unos tama√±o de 1 y 8 billones de par√°metros.

### Resultados en escabilidad

Si bien el paper presenta un an√°lisis muy detallado de resultados, a mi los m√°s interesantes me parecen aquellos relacionados con la escabilidad. En el art√≠culo se presenta la siguiente figura, la cual consideron muy interesante:

![Generation process](assets/posts/2025-09-16-llada/scalability.png){: width="400" height="400" }

En ella podemos ver como se presenta la idea de que conforme aumenta la cantidad de computo invertido en entrenamiento (usualmente eso es incrementar el n√∫mero de √©pocas si dejamos fijo el dataset), la escalabilidad de LLaDA parece escalar mejor que los modelos autorregresivos en m√∫ltiples tareas.

Esto conecta con un articulo reciente llamado [Diffusion Language Models are Super Data Learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac#23bd8f03a86680699f7ad6fa98caa3d2), d√≥nde se presenta la idea de que para una misma cantidad de datos, los modelos basados en diffusion son capaces de extraer m√°s jugo de los datos que los modelos autorregresivos. Como prueba, en el art√≠culo presentan la siguiente imagen:

![Generation process](assets/posts/2025-09-16-llada/diffusion_vs_ar_grid.png){: width="400" height="400" }

Aqu√≠ podemos ver como, manteniendo el mismo n√∫mero de muestras, los modelos de diffusion pueden seguir extreyendo informaci√≥n de los datos durante varias √©pocas, mientras que los modelos autorregresivos acaban cayendo en overfitting. Esto es gracias a reusar datos bajo distintos niveles de ruido. In autoregressive training, a sample always yields the same gradient. In diffusion, the same data comes with different corruption levels ‚Üí different gradients. You squeeze more juice out of each sample without overfitting, almost like a built-in regularization hack.

## Implementaci√≥n propia

Debido a que me interesaba este paper, y a que mis experiencias anteriores no fueron del todo satisfactorias con los modelos de diffusion aplicadas al texto, se me ocurri√≥ probar este m√©todo y hacer una implementaci√≥n custom.

Para ello, utilic√© el modelo pre-entrenado de [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) (modelo transformer no causal y con 66M par√°metros) y el dataset de [tinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) (un dataset sencillito que contiene peque√±as historias generadas por GPT-3.5 y GPT4). A continuaci√≥n os dejo una muestra de su funcionamiento a la hora de generar una nueva historia:

![Generation process](assets/posts/2025-09-16-llada/generation.gif){: width="400" height="400" }

La verdad que para un modelo tan peque√±o, y un entrenamiento tan r√°pido lo considero todo un √©xito üòÇ. Os dejo por aqu√≠ el [repositorio](https://github.com/javiersgjavi/diffusion-llm/tree/main) por si quereis echarle un vistazo.

## Conclusiones

Despu√©s de meses de experimentaci√≥n frustrante con modelos de difusi√≥n para texto y de implementar LLaDA desde cero, este trabajo representa un punto de inflexi√≥n en c√≥mo pensamos sobre la generaci√≥n de texto con modelos de difusi√≥n.

La lecci√≥n m√°s importante que extraigo de LLaDA es que la simplicidad conceptual supera a la complejidad t√©cnica. Mientras que mis intentos anteriores se perd√≠an trabajando con embeddings continuos y espacios latentes, LLaDA aborda el problema directamente: trabaja con tokens discretos desde el principio. Esto no solo resuelve el problema de las regiones inv√°lidas que tanto me frustr√≥, sino que hace el modelo m√°s interpretable y f√°cil de debuggear.

Sin embargo, LLaDA no es una panacea. El proceso de generaci√≥n requiere realizar T predicciones secuenciales, y si este n√∫mero T no es significativamente menor que la longitud de la secuencia a generar, las ventajas en eficiencia son debatibles respectos a los modelos autorregresivos. Adem√°s, estos cuentan actualmente con m√∫ltiples optimizaciones como el KV caching que LLaDA no puede aprovechar por su naturaleza no causal.

Implementar LLaDA me ense√±√≥ que a veces la soluci√≥n m√°s elegante es la que cuestiona las suposiciones fundamentales que todos damos por sentadas. Durante meses asum√≠ que trabajar con embeddings continuos era la √∫nica forma de aplicar difusi√≥n al texto. LLaDA me mostr√≥ que estaba equivocado. La diferencia no fue solo t√©cnica, sino conceptual: mientras yo trataba de forzar el texto en el molde de la difusi√≥n continua, LLaDA adapt√≥ la difusi√≥n al molde natural del texto discreto.

LLaDA no es solo un avance t√©cnico, sino una invitaci√≥n a cuestionar los paradigmas establecidos en NLP. Te animo a experimentar con LLaDA y a cuestionar otras suposiciones que damos por sentadas en el campo. La pr√≥xima gran innovaci√≥n podr√≠a estar esperando que alguien se atreva a preguntar "¬øpor qu√© no?".

## Referencias

### Papers Fundamentales

- [Latent Diffusion for Language Generation](https://arxiv.org/abs/2212.09462) - Justin Lovelace et al.
- [TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings](https://arxiv.org/abs/2402.16606)
- [Continuous diffusion for categorical data](https://arxiv.org/abs/2211.15089)
- [LLaDA: Large Language Diffusion Models with Masking](https://arxiv.org/abs/2404.07199) - Paper original de LLaDA

### Papers de Referencia Adicionales

- [Austin et al. - Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/abs/2107.03006)
- [Jinjie Ni, & the team. (2025). Diffusion Language Models are Super Data Learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac#23bd8f03a86680699f7ad6fa98caa3d2)


{% include comments.html %}