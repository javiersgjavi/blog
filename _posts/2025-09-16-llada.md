---
title: (DRAFT) Large Language Masked Diffusion Models (LLaDA)
date: 2025-09-16 00:02:30 +/-1200
categories: [Explanation]
tags: [diffusion, llm]     ## TAG names should always be lowercase
math: true
image:
    path: assets/posts/2025-09-16-llada/portada.png
    alt: Text generated by a Masked Diffusion Model
---

# LLaDA: Diffusion Models que Finalmente Funcionan para Texto

## Introducción Personal y Motivación

A principios de este año estuve trabajando intensamente con modelos de difusión y modelos de consistencia aplicados a la generación de embeddings en espacios latentes. Mi objetivo era ambicioso: conseguir que estos enfoques funcionaran para texto de manera similar a como revolucionaron la generación de imágenes.

Me inspiré principalmente en tres papers fundamentales que exploraban diferentes aproximaciones al problema:

1. **"Latent Diffusion for Language Generation"** de Justin Lovelace
2. **"TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings"**
3. **"Continuous diffusion for categorical data"**

Sin embargo, a pesar de seguir estas investigaciones y probar múltiples combinaciones y variaciones propias, los resultados que obtuve no tenían la calidad que esperaba y presentaban una complejidad considerable tanto en implementación como en entrenamiento.

{: .prompt-warning }
> **Advertencia sobre la complejidad**: Los modelos de difusión para texto han sido históricamente mucho más desafiantes que para imágenes. Mi experiencia personal lo confirma: meses de trabajo intenso con resultados frustrantes.

Después de varios meses de experimentación, logré desarrollar un modelo de consistencia que funcionaba parcialmente, pero no al nivel que deseaba. Los modelos de difusión tradicionales que implementé simplemente no llegaban a generar texto coherente - algo que me frustraba enormemente dado el éxito que estos modelos habían tenido en otros dominios.

Fue entonces cuando me topé con el paper de LLaDA (Large Language Diffusion Models with Masking). Lo que inmediatamente me llamó la atención fue su claridad conceptual, su intuitividad y, sobre todo, su enfoque directo al problema. A diferencia de mis intentos anteriores que se perdían en complejidades técnicas, LLaDA presentaba una solución elegante y comprensible.

Es importante mencionar que LLaDA no aborda específicamente la generación de embeddings - el problema concreto en el que yo estaba trabajando. Sin embargo, me di cuenta de que mi enfoque de trabajar con embeddings era en realidad una forma de simplificar la tarea, evitando lidiar directamente con la naturaleza discreta del texto. El paper de LLaDA me mostró que era posible abordar el problema de manera directa, trabajando con tokens discretos mediante enmascaramiento, y que este enfoque podría ser perfectamente aplicable a mi tarea original. De ahí mi inmediato interés.

La diferencia fue notable: mientras que mis implementaciones previas requerían semanas de debugging y ajuste de hiperparámetros, pude hacer funcionar LLaDA con relativamente poco trabajo de implementación. Esto no solo validó el enfoque, sino que me hizo reflexionar sobre la importancia de la simplicidad conceptual en el diseño de modelos.

En este post voy a compartir todo lo que he aprendido sobre LLaDA, un modelo que desafía uno de los paradigmas más establecidos en NLP: la generación autoregresiva. Más importante aún, voy a explicar por qué este enfoque funciona donde otros (incluidos los míos) habían fallado.

{: .prompt-info }
Nota breve y especulativa: sospecho que el enfoque de LLaDA —difusión discreta con enmascarado y predicción paralela de tokens— está relacionado con lo que Google DeepMind denomina "Gemini Diffusion". La descripción pública menciona generación por bloques e iteración con refinamiento, señales que encajan con esta familia de métodos. Aún así, es solo una hipótesis hasta que publiquen el paper técnico. Referencia: [deepmind.google/models/gemini-diffusion](https://deepmind.google/models/gemini-diffusion/).

### Problemas que resuelve LLaDA

#### Generación Autoregresiva vs Difusión

Una de las contribuciones más importantes del paper de LLaDA es cuestionar una suposición fundamental que la comunidad de NLP había aceptado casi sin debate: **¿es la generación autoregresiva realmente necesaria para crear modelos de lenguaje inteligentes?**

Los autores argumentan algo que, en retrospectiva, parece obvio pero que pocos habían verbalizado claramente: la habilidad de generar texto coherente no es específica de los modelos autoregresivos. Esta capacidad se debe más bien a tres factores fundamentales:

1. **La arquitectura Transformer** y su capacidad de modelar dependencias complejas
2. **La cantidad masiva de datos** utilizados durante el entrenamiento
3. **El proceso de entrenamiento generativo** que enseña al modelo a capturar patrones estadísticos

Por lo tanto, capacidades emergentes como el aprendizaje en contexto (in-context learning) y la habilidad de seguir instrucciones tampoco deberían ser exclusivas de los modelos autoregresivos.

{: .prompt-info }
> **Reflexión personal**: Cuando leí esta argumentación por primera vez, me pareció una de esas ideas que son "obviamente correctas" una vez que alguien las explicita, pero que nadie había cuestionado seriamente antes.

##### Los Problemas Inherentes de la Generación Autoregresiva

**Costo Computacional Prohibitivo**

El primer problema que LLaDA aborda es la ineficiencia fundamental de la generación autoregresiva. Cada vez que queremos generar un nuevo token, debemos:

1. Volver a computar la representación de toda la secuencia anterior
2. Realizar un paso completo por la red neuronal
3. Repetir este proceso para cada token individual

Esto significa que para generar una secuencia de 100 tokens, necesitamos realizar 100 pasadas por la red. Es como si cada vez que quisiéramos añadir una palabra a un párrafo, tuviéramos que releer todo el texto desde el principio.

**Limitaciones Direccionales del Razonamiento**

El segundo problema es más sutil pero igualmente importante: la generación autoregresiva solo puede procesar texto de izquierda a derecha. Esta restricción direccional limita la capacidad de razonamiento en tareas específicas donde el contexto bidireccional es crucial.

Un ejemplo perfecto son las tareas de "reversión" que menciona el paper: si le pides a un modelo autoregresivo que complete un poema hacia atrás, tendrá enormes dificultades porque nunca ha aprendido a razonar en esa dirección. Es como si hubiera aprendido a leer pero nunca a escribir de derecha a izquierda.

**Eficiencia de Aprendizaje Limitada**

Finalmente, y esto es algo que me parece particularmente interesante, los autores argumentan que los modelos de difusión tienen una **mayor capacidad para extraer aprendizaje de la misma cantidad de datos**. Esto se debe a que pueden ver el contexto completo (bidireccional) durante el entrenamiento, lo que les permite:

- Capturar patrones más complejos en los datos
- Ser más eficientes durante el entrenamiento
- Escalar mejor con la misma cantidad de recursos

{: .prompt-warning }
> **Importante**: Esta última afirmación sobre eficiencia de aprendizaje es una de las más controvertidas del paper. Aunque los resultados experimentales la respaldan, aún necesitamos más investigación para confirmarla completamente.

En mi experiencia implementando tanto modelos autoregresivos como LLaDA, la diferencia en la velocidad de convergencia durante el entrenamiento fue notable. LLaDA parecía "entender" los patrones en los datos más rápidamente, aunque esto podría deberse a múltiples factores.

#### Generación con Difusión: Los Intentos Anteriores

Para entender por qué LLaDA representa un avance tan significativo, es crucial comprender los obstáculos que enfrentaron los intentos previos de aplicar difusión al texto. La historia de estos experimentos está llena de frustraciones técnicas que conozco de primera mano.

##### El Problema Fundamental: Discreto vs Continuo

Los modelos de difusión originales, como DDPM, fueron diseñados para trabajar con **espacios latentes continuos**. Esto funciona perfectamente para imágenes, donde cada píxel puede tomar cualquier valor en un rango continuo (por ejemplo, de 0 a 255 en RGB). Sin embargo, el texto es inherentemente **discreto**: cada token es una entidad específica del vocabulario, no un punto en un espacio continuo.

Los primeros intentos de aplicar difusión al texto trataron de sortear este problema fundamental moviéndose del espacio discreto de tokens a espacios continuos de embeddings. La lógica parecía sólida: si podemos representar cada token como un vector de embeddings, entonces podemos aplicar difusión gaussiana a estos vectores continuos (Diffusion-LM Improves Controllable Text Generation). 

También hubo algunos enfoques los cuales trataron de adaptar el proceso de poner ruido gausiano en un espacio continuo a uno similar en un espacio discreto. (Structured Denoising Diffusion Models in Discrete State-Spaces)

**Pero los resultados no fueron los esperados.**

##### Mi Experiencia Personal: El Problema de las Regiones Inválidas

Durante mis meses experimentando con latent diffusion models para generar embeddings de texto, me topé exactamente con el problema que se describe en el paper de "Reflected Diffusion Models": **el problema de las regiones inválidas del espacio latente**.

{: .prompt-warning }
> **El problema de las regiones inválidas**: Durante el proceso de eliminación de ruido, los modelos pueden cometer errores que los llevan a regiones del espacio latente que no corresponden a ningún token o secuencia válida.

Cuando trabajas con píxeles en imágenes, este problema es manejable. Si el modelo predice un valor de píxel ligeramente incorrecto, obtienes una imagen un poco borrosa o con artefactos, pero sigue siendo una imagen interpretable. Sin embargo, cuando trabajas con embeddings que representan secuencias de tokens, los errores en la eliminación de ruido pueden llevarte a vectores que:

1. **No corresponden a ningún token real** del vocabulario
2. **Representan combinaciones semánticamente inconsistentes**
3. **Son imposibles de mapear de vuelta al espacio discreto** de manera coherente

En mi implementación, esto se traducía en generaciones que parecían "casi correctas" pero que al intentar convertirlas de vuelta a tokens discretos, producían secuencias sin sentido o completamente incoherentes.

##### El Escalamiento Limitado

Además de estos problemas técnicos, los enfoques de difusión continua para texto mostraron **limitaciones severas de escalamiento**. Los modelos que implementé, aunque funcionaban en ejemplos pequeños, no lograban escalar a vocabularios grandes o secuencias largas de manera efectiva. La complejidad de mantener la coherencia semántica en el espacio continuo crecía exponencialmente con el tamaño del problema.

##### El Nacimiento de los Masked Diffusion Models

Fue en este contexto de frustración con los enfoques continuos que nacieron los **Masked Diffusion Models**. Aunque técnicamente no son exactamente iguales a los modelos de difusión tradicionales, comparten una idea fundamental similar:

**En lugar de partir de ruido gaussiano, partimos de tokens completamente enmascarados.**

La analogía es elegante:
- **DDPM tradicional**: Ruido gaussiano → Datos limpios  
- **Masked Diffusion**: Tokens enmascarados → Texto real

El proceso de "eliminación de ruido" se convierte en un proceso de **desenmascaramiento progresivo**, donde:

1. Comenzamos con una secuencia completamente enmascarada: `[MASK] [MASK] [MASK] [MASK]`
2. Gradualmente vamos revelando tokens: `[MASK] the [MASK] [MASK]`
3. Continuamos hasta obtener texto completo: `Hello the world today`

{: .prompt-info }
> **Insight clave**: Este enfoque evita completamente el problema de las regiones inválidas porque siempre trabajamos directamente en el espacio discreto de tokens. No hay conversión problemática entre espacios continuos y discretos.

Esta transición conceptual de "difusión gaussiana" a "difusión discreta con enmascaramiento" fue el breakthrough que permitió que modelos como LLaDA finalmente funcionaran donde otros habían fallado.

## ¿Cómo Funciona LLaDA?

Ahora que hemos establecido por qué los enfoques anteriores fallaron y cómo surgió la idea de los Masked Diffusion Models, es momento de sumergirnos en los detalles técnicos de LLaDA. Lo que hace brillante a este enfoque es su simplicidad conceptual: toma la intuición exitosa de la difusión y la adapta elegantemente al mundo discreto del texto.

### Masked Diffusion: La Innovación Central

LLaDA implementa lo que podríamos llamar "difusión discreta" - un proceso que mantiene la estructura iterativa de los modelos de difusión tradicionales pero opera completamente en el espacio de tokens discretos.

La diferencia fundamental es elegante en su simplicidad:

- **DDPM tradicional**: \\( \text{Ruido gaussiano} \xrightarrow{\text{eliminación de ruido}} \text{Datos limpios} \\)
- **LLaDA**: \\( \text{Tokens enmascarados} \xrightarrow{\text{desenmascaramiento}} \text{Texto real} \\)

#### Proceso Forward (Corrupción): 

En lugar de añadir ruido gaussiano, LLaDA gradualmente enmascara tokens con probabilidad creciente a lo largo de los pasos de difusión.

Matemáticamente, para una secuencia de datos \\(x_0 \sim p_{\text{data}}\\), el proceso de corrupción se define como:

$$
\begin{equation}
q(x_t|x_0) = \text{Mask}(x_0, t)
\label{eq:forward_mask}
\end{equation}
$$

Donde el proceso de enmascaramiento funciona de la siguiente manera:

1. **Muestreamos el nivel de corrupción**: \\(t \sim \mathcal{U}(0,1]\\)
2. **Generamos una máscara aleatoria**: \\(M \sim \mathcal{U}(0,1]^{|x_0|}\\) con la misma longitud que \\(x_0\\)
3. **Aplicamos el enmascaramiento**:

$$
\begin{equation}
x_t[i] = \begin{cases}
\text{[MASK]} & \text{si } M[i] < t \\
x_0[i] & \text{si } M[i] \geq t
\end{cases}
\label{eq:masking_rule}
\end{equation}
$$

Esta formulación es análoga al proceso forward de DDPM, pero en lugar de añadir ruido gaussiano, aplicamos enmascaramiento estocástico. El parámetro \\(t\\) controla la intensidad de la corrupción: cuando \\(t \rightarrow 0\\), pocos tokens se enmascaran; cuando \\(t \rightarrow 1\\), la mayoría de tokens se convierten en \\(\text{[MASK]}\\).

Durante el entrenamiento, lo que vamos a intentar optimizar es la siguiente función de pérdidas:

$$
\begin{equation}
\mathcal{L}  = - \frac{1}{t*L} \sum^L_{i=1} {\mathbf{1}[x_t^i = M] \log p_\theta(x^i_0|x_t)} 
\label{eq:loss}
\end{equation}
$$

Qué básicamente lo que hace es comparar las distribuciones predichas por el modelo de cada uno de los tokens de la secuencia, pero solámente para aquellas posiciones que estaban inicialmente enmascaradas. Es importante comentar que hay un factor de suavizado en función de t, ya que no se penaliza igual al modelo por un fallo cuando está la secuencia totalmente enmascarada, y por lo tanto, por definir ($$ t=1 $$), que cuando estamos ya refinando los últimos tokens ($$ t=0.05 $$)

![Generation process](assets/posts/2025-09-16-llada/predictor.png){: width="400" height="400" }

#### Proceso Reverse (Generación): 

En lugar de predecir ruido para eliminarlo, el modelo predice directamente qué tokens deberían ocupar las posiciones enmascaradas, y lo hace simultáneamente para múltiples posiciones. Esto lo hacemos de manera iterativa y gradual, recorriendo los posibles valores de $$ t $$, que estaban en un rango de $$ t \in [0,1] $$ desde al máximo al mínimo en una cantidad de $$ T $$ pasos que debemos definir.

El proceso reverse se formaliza como:

$$
\begin{equation}
p_\theta(x_{t-1}|x_t) = \text{Predict}(x_t, t) \rightarrow \hat{x}_t
\hat{x}_0 = \text{arg max} p_\theta(x_0 | x_t)
\label{eq:reverse_predict}
\end{equation}
$$

El modelo \\(\theta\\) toma como entrada la secuencia parcialmente enmascarada \\(x_t\\), y produce predicciones simultáneas de la distribución de probabilidad de los tokens para todas las posiciones enmascaradas. Una posible forma de recuperar los tokens finalmente podría ser muestrear los tokens con mayor probabilidad de estas distribuciones para cada posición, tal y como se muestra en la Ecuación \ref{eq:reverse_predict}, aunque se podrían aplicar otros métodos, como un muestreo desde la distribución. Crucialmente, esto permite **generación paralela** de múltiples tokens, contrastando con la naturaleza secuencial de los modelos autoregresivos.

Una vez obtenidas las predicciones \\(\hat{x}_t\\), necesitamos aplicar una estrategia de **re-enmascaramiento** para decidir qué tokens mantener y cuáles volver a enmascarar en el siguiente paso, ya que de la red vamos a obtener la distribución de todos los tokens, pero el proceso de generación debe de ser gradual e ir refinando la predicción poco a poco.

En cuanto a los métodos para ello, hay muchas técnicas que se puede aplicar:

    - Ir seleccionando gradualmente de forma probabilística que tokens nos quedamos, de forma que si uno ya ha sido fijado se mantenga siempre así y cada vez la secuencia esté más descubierta.

    - El paper también propone un método un poco más inteligente que propone que se enmascara de forma probabilística, como el anterior, pero dando prioridad a aquellos tokens que tienen una distribución más uniforme, y por lo tanto, menor confianza en la predicción.

De nuevo, una de las cosas interesantes del modelo de diffusion versus un modelo autorregresivo es la eficiencia. Si bien para generar una secuencia de N tokens hacen falta N pasos por la red para un modelo autorregresivo, en un modelo de difusión solamente hacen falta T pasos por la red. Si bien es cierto que estos modelos se beneficián con mayores valores de T, para poder refinar la predicción, si llegamos a escalas en las que N >> T, empezamos a encontrar beneficios en la velocidad de generación.

#### Otras consideraciones

Es importante destacar que si la explicación anterior es la base de LLaDA, también es posible extender el entrenamiento a una posible fase de Fine-Tuning en el cual el modelo tenga un prompt y enmascaramos solo la región de la secuencia de respuesta destinada a la respuesta generada por el LLM, similar al Fine-tuning que se hace con modelos autorregresivos para que aprendan a aceptar prompts, el in-context learning o following instructions.

La extensión a estos métodos es bastante sencilla, simplemente teniendo que modelar el problema como uno de generación condicional con un prompt de entrada $$ p_\theta (x_0 | p_0, x_t) $$, dónde $$ p_0 $$ sería la secuencia referente al prompt. Sin embargo, en este post no vamos a cubrir tanto esa parte, ya que creo que las bases del método han quedado más o menos claras y no quiero alargar aún más la extensión de este post.

![Generation process](assets/posts/2025-09-16-llada/prompt.png){: width="400" height="400" }

## Resultados de LLaDA

### Modelo utilizado

Lo primero que hay que comentar es que debido al tipo de generación basada en máscaras y teniendo en cuenta toda la secuencia a la vez, en lugar de izquierda a derecha como los modelos autorregresivos, un modelo basado en LLaDA debería de implementarse con modelos basados en transformers no causales, dónde estos puedan presetar atención para cada token a cualquier otro de la secuencia. Por lo tanto, tampoco es posible aplicar KV caching.

En el paper, ellos implementan dos modelo transformer básicos, similar a muchos otros LLMs, con unos tamaño de 1 y 8 billones de parámetros.

### Resultados en escabilidad

Si bien el paper presenta un análisis muy detallado de resultados, a mi los más interesantes me parecen aquellos relacionados con la escabilidad. En el artículo se presenta la siguiente figura, la cual consideron muy interesante:

![Generation process](assets/posts/2025-09-16-llada/scalability.png){: width="400" height="400" }

En ella podemos ver como se presenta la idea de que conforme aumenta la cantidad de computo invertido en entrenamiento (usualmente eso es incrementar el número de épocas si dejamos fijo el dataset), la escalabilidad de LLaDA parece escalar mejor que los modelos autorregresivos en múltiples tareas.

Esto conecta con un articulo reciente llamado [Diffusion Language Models are Super Data Learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac#23bd8f03a86680699f7ad6fa98caa3d2), dónde se presenta la idea de que para una misma cantidad de datos, los modelos basados en diffusion son capaces de extraer más jugo de los datos que los modelos autorregresivos. Como prueba, en el artículo presentan la siguiente imagen:

![Generation process](assets/posts/2025-09-16-llada/diffusion_vs_ar_grid.png){: width="400" height="400" }

Aquí podemos ver como, manteniendo el mismo número de muestras, los modelos de diffusion pueden seguir extreyendo información de los datos durante varias épocas, mientras que los modelos autorregresivos acaban cayendo en overfitting. Esto es gracias a reusar datos bajo distintos niveles de ruido. In autoregressive training, a sample always yields the same gradient. In diffusion, the same data comes with different corruption levels → different gradients. You squeeze more juice out of each sample without overfitting, almost like a built-in regularization hack.

## Implementación propia

Debido a que me interesaba este paper, y a que mis experiencias anteriores no fueron del todo satisfactorias con los modelos de diffusion aplicadas al texto, se me ocurrió probar este método y hacer una implementación custom.

Para ello, utilicé el modelo pre-entrenado de [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) (modelo transformer no causal y con 66M parámetros) y el dataset de [tinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) (un dataset sencillito que contiene pequeñas historias generadas por GPT-3.5 y GPT4). A continuación os dejo una muestra de su funcionamiento a la hora de generar una nueva historia:

![Generation process](assets/posts/2025-09-16-llada/generation.gif){: width="400" height="400" }

La verdad que para un modelo tan pequeño, y un entrenamiento tan rápido lo considero todo un éxito 😂. Os dejo por aquí el [repositorio](https://github.com/javiersgjavi/diffusion-llm/tree/main) por si quereis echarle un vistazo.

## Conclusiones

Después de meses de experimentación frustrante con modelos de difusión para texto y de implementar LLaDA desde cero, este trabajo representa un punto de inflexión en cómo pensamos sobre la generación de texto con modelos de difusión.

La lección más importante que extraigo de LLaDA es que la simplicidad conceptual supera a la complejidad técnica. Mientras que mis intentos anteriores se perdían trabajando con embeddings continuos y espacios latentes, LLaDA aborda el problema directamente: trabaja con tokens discretos desde el principio. Esto no solo resuelve el problema de las regiones inválidas que tanto me frustró, sino que hace el modelo más interpretable y fácil de debuggear.

Sin embargo, LLaDA no es una panacea. El proceso de generación requiere realizar T predicciones secuenciales, y si este número T no es significativamente menor que la longitud de la secuencia a generar, las ventajas en eficiencia son debatibles respectos a los modelos autorregresivos. Además, estos cuentan actualmente con múltiples optimizaciones como el KV caching que LLaDA no puede aprovechar por su naturaleza no causal.

Implementar LLaDA me enseñó que a veces la solución más elegante es la que cuestiona las suposiciones fundamentales que todos damos por sentadas. Durante meses asumí que trabajar con embeddings continuos era la única forma de aplicar difusión al texto. LLaDA me mostró que estaba equivocado. La diferencia no fue solo técnica, sino conceptual: mientras yo trataba de forzar el texto en el molde de la difusión continua, LLaDA adaptó la difusión al molde natural del texto discreto.

LLaDA no es solo un avance técnico, sino una invitación a cuestionar los paradigmas establecidos en NLP. Te animo a experimentar con LLaDA y a cuestionar otras suposiciones que damos por sentadas en el campo. La próxima gran innovación podría estar esperando que alguien se atreva a preguntar "¿por qué no?".

## Referencias

### Papers Fundamentales

- [Latent Diffusion for Language Generation](https://arxiv.org/abs/2212.09462) - Justin Lovelace et al.
- [TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings](https://arxiv.org/abs/2402.16606)
- [Continuous diffusion for categorical data](https://arxiv.org/abs/2211.15089)
- [LLaDA: Large Language Diffusion Models with Masking](https://arxiv.org/abs/2404.07199) - Paper original de LLaDA

### Papers de Referencia Adicionales

- [Austin et al. - Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/abs/2107.03006)
- [Jinjie Ni, & the team. (2025). Diffusion Language Models are Super Data Learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac#23bd8f03a86680699f7ad6fa98caa3d2)


{% include comments.html %}