---
title: (DRAFT) Large Language Masked Diffusion Models (LLaDA)
date: 2025-09-16 00:02:30 +/-1200
categories: [Explanation]
tags: [diffusion, llm]     ## TAG names should always be lowercase
math: true
image:
    path: assets/posts/2025-09-16-llada/portada.png
    alt: Text generated by a Masked Diffusion Model
---

# LLaDA: Diffusion Models que Finalmente Funcionan para Texto

## Introducción Personal

A principios de este año, me estuve trabajando en aplicar **Modelos de Difusión y Consistencia** a la generación de *embeddings* en espacios latentes en el contexto de NLP, buscando replicar el éxito que me dieron en la imputación de series temporales de mi tesis. Quería trasladar ese valor a casos de uso internos en Avature en los que podría ser muy interesante utilizar modelos generativos.

Me inspiré en investigaciones como:

1. [Latent Diffusion for Language Generation](https://arxiv.org/pdf/2212.09462)
2. [TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings](https://arxiv.org/pdf/2402.19097)
3. [Continuous diffusion for categorical data](https://arxiv.org/pdf/2211.15089)

A pesar de la intensidad y meses de experimentación, los resultados eran frustrantes. Los modelos de difusión para texto han sido **históricamente más desafiantes** que para imágenes, y mi experiencia lo confirmó: la calidad de las generaciones era baja y la implementación, innecesariamente compleja. Los enfoques que probé no lograban una coherencia a la altura, algo frustrante dado su éxito en otros dominios. Aunque acabé con un modelo de consistencia (quizás este tipo modelos podemos tratarlos en un futuro post!) que daba algunos resultados, seguía sin ser satisfactorio. Por este motivo abandoné este enfoque al menos de forma temporal.


{: .prompt-warning }
> **Advertencia sobre la complejidad**: Los modelos de difusión para texto han sido históricamente mucho más desafiantes que para imágenes. Mi experiencia personal lo confirma: meses de trabajo intenso con resultados frustrantes.

### El Giro de LLaDA

Sin embargo, hace unas semanas me topé con el *paper* **LLaDA** ([Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)). Lo que me impactó fue su **claridad conceptual y elegancia**. Mientras yo me perdía en la lucha del espacio continuo y los *embeddings*, LLaDA propuso una solución directa.

Me di cuenta de que mi enfoque de trabajar con *embeddings* era una forma de evitar la naturaleza **discreta** del texto. LLaDA me demostró que era posible abordar el problema de frente, trabajando con **tokens discretos mediante enmascaramiento**. La diferencia fue notable: pude hacer funcionar LLaDA con relativamente poco trabajo de implementación, frente a las semanas de *debugging* previas.

En este post, compartiré por qué este modelo funciona donde otros (incluidos los míos) fallaron, desafiando el paradigma de la **generación autoregresiva**. Y más abajo os comparto un enlace al repositorio donde he podido hacer una implementación rápida de este

{: .prompt-info }
Nota especulativa: sospecho que el enfoque de LLaDA (difusión discreta con enmascarado y predicción paralela de tokens) tiene una relación con lo que Google DeepMind denomina "Gemini Diffusion," que describe una generación por bloques e iteración con refinamiento. Referencia: [deepmind.google/models/gemini-diffusion](https://deepmind.google/models/gemini-diffusion/).




## Problemas que Resuelve LLaDA: El Ataque al Paradigma AR

LLaDA no es solo otro modelo; es un desafío directo a la **generación autoregresiva (AR)**, el dogma central de los LLMs modernos. Los autores del *paper* cuestionan un supuesto fundamental: la habilidad de generar texto inteligente no es inherente al orden secuencial, sino a la **Arquitectura Transformer**, la **cantidad de datos** y la **naturaleza del entrenamiento generativo**.

### Las Cuatro Grietas de la Generación Autoregresiva

Si miramos de cerca, la generación AR presenta fallos estructurales que LLaDA busca superar:

1.  **Costo Computacional Prohibitivo y Latencia:** La ineficiencia es la más obvia. Para generar una secuencia de $N$ tokens, el modelo debe realizar $N$ pasadas por la red. Es decir, cada vez que predice un nuevo token, debe recomputar la representación de **toda la secuencia anterior**. Esto convierte el *decoding* en un proceso lento y secuencial, limitando la escalabilidad de la inferencia.

2.  **Limitaciones Direccionales del Razonamiento:** Un modelo AR solo aprende a razonar de izquierda a derecha. Esta restricción direccional es un obstáculo crucial en tareas que inherentemente requieren contexto bidireccional, como mencionan en el *paper* con las tareas de "reversión" de texto. El modelo no puede "mirar hacia adelante" para asegurar la coherencia de su elección actual con el futuro.

3.  **Divergencia y Acumulación de Errores (El Problema de LeCun):** Este es, para mí, el más grave. Como bien ha señalado [Yann LeCun](https://www.youtube.com/watch?v=ETZfkkv6V7Y), si el modelo comete un error al predecir un token, ese error se propaga inmediatamente: contamina el contexto para el siguiente paso. Esta alteración lleva a una cadena de fallos que crecen exponencialmente. **Un modelo AR no tiene un mecanismo para corregir un error una vez cometido.** Por el contrario, los modelos basados en difusión permiten el **refinamiento iterativo**.

    ![Presentación LeCunn](assets/posts/2025-09-16-llada/lecun_llms.png){: width="800" height="800" }

4.  **Eficiencia de Aprendizaje Limitada:** El entrenamiento autoregresivo, al ver una muestra siempre de la misma forma, cae rápidamente en *overfitting*. Los modelos de difusión, al reutilizar la misma muestra bajo diferentes niveles de corrupción o ruido, demuestran una **mayor capacidad para extraer aprendizaje de la misma cantidad de datos**. LLaDA escala mejor porque aprende más eficientemente.

***

### Generación con Difusión: Por Qué Fallaron los Intentos Anteriores

Para entender el avance de LLaDA, debemos recordar qué problemas técnicos tuvieron los intentos previos de aplicar difusión al texto.

#### El Conflicto Fundamental: Discreto vs. Continuo

Los modelos de difusión, como DDPM, nacieron y prosperaron en el dominio de las imágenes porque trabajan con **espacios latentes continuos** (los valores de los píxeles). El texto, sin embargo, es **inherentemente discreto**: cada token es una entidad fija en un vocabulario.

Los primeros enfoques intentaron forzar esta compatibilidad:
1.  **Difusión en *Embeddings* (Continuos):** Aplicar difusión gaussiana a los vectores de *embeddings* de los tokens (ejemplos como [TEncDM](https://arxiv.org/pdf/2402.19097)).

2.  **Adaptaciones Discretas:** Intentar crear procesos de ruido discretos similares al gaussiano (ejemplo: [Structured Denoising Diffusion Models](https://arxiv.org/pdf/2107.03006)).

Apesar de esto, parece que los resultados no fueron los esperados. Y durante 2023 y 2024, muchos veían en los modelos de difusión aplicados al texto una promesa que no parecía despegar del todo.

#### Mi Frustración Personal: Las Regiones Inválidas

Mi experiencia de meses se topó de lleno con el problema descrito en [Reflected Diffusion Models](https://arxiv.org/pdf/2304.04740): **el problema de las regiones inválidas del espacio latente**.

{: .prompt-warning }
> **El problema de las regiones inválidas**: Durante el proceso de eliminación de ruido, los errores pueden llevar al modelo a regiones del espacio latente que **no corresponden a ningún token o secuencia válida**.

Cuando trabajamos con imágenes, un error es un píxel borroso. Sin embargo, cuando trabajas con embeddings que representan secuencias de tokens, los errores en la eliminación de ruido pueden llevarte a vectores que:

1. **No corresponden a ningún token real** del vocabulario.
2. **Representan combinaciones semánticamente inconsistentes**.
3. **Son imposibles de mapear de vuelta al espacio discreto** de manera coherente.

En mi implementación, esto se traducía en generaciones que parecían "casi correctas" pero que producían secuencias sin sentido o completamente incoherentes.

![Representación de muestras de datos que cruzaban hacia regiones inválidas](assets/posts/2025-09-16-llada/reflected.png){: width="800" height="800" }

### Masked Diffusion Models

La solución de LLaDA es simple: **abandonar el ruido gaussiano y el espacio continuo**. Es un cambio conceptual profundo:

**En lugar de partir de ruido gaussiano, partimos de tokens completamente enmascarados.**

La analogía se simplifica:
* **Difusión Continua (DDPM):** $\text{Ruido gaussiano} \rightarrow \text{Datos limpios}$
* **Difusión Discreta (LLaDA):** $\text{Tokens enmascarados} \rightarrow \text{Texto real}$

El proceso de "eliminación de ruido" se convierte en **desenmascaramiento progresivo e iterativo**. Este enfoque evita por completo el problema de las regiones inválidas porque el modelo **siempre opera en el espacio discreto de tokens**. Este fue el *breakthrough* que permitió que LLaDA funcionara donde todos los intentos continuos previos habían fallado.





## ¿Cómo Funciona LLaDA?

Ahora que entendemos por qué los enfoques continuos fallaron, podemos sumergirnos en la brillantez de LLaDA. Su genialidad reside en su **simplicidad conceptual**: toma la exitosa intuición de la difusión y la adapta con elegancia al mundo discreto. LLaDA implementa lo que llamamos **difusión discreta**, manteniendo la estructura iterativa de DDPM pero operando exclusivamente en el espacio de *tokens*.

### Masked Diffusion

#### Proceso Forward (Corrupción)

En lugar de añadir ruido gaussiano como en los modelos de difusión tradicionales, LLaDA aplica **enmascaramiento estocástico**, corrompiendo la secuencia original $x_0$ al convertir gradualmente *tokens* en tokens enmascarados (`[MASK]`) a lo largo de los pasos de difusión.

Matemáticamente, para una secuencia de datos $x_0 \sim p_{\text{data}}$, el proceso de corrupción se define por una función de enmascaramiento:

$$
\begin{equation}
q(x_t|x_0) = \text{Mask}(x_0, t)
\label{eq:forward_mask}
\end{equation}
$$

El enmascaramiento depende del nivel de corrupción $t \in (0,1]$ y durante el entrenamiento se aplica de la siguiente manera:

1.  **Muestreo:** Se muestrea el nivel de corrupción $t \sim \mathcal{U}(0,1]$ y una máscara aleatoria $M \sim \mathcal{U}(0,1]^{|x_0|}$.
2.  **Aplicación:** El *token* $x_0^i$ se convierte en $\text{[MASK]}$ si $M^i$ es menor que $t$.

$$
\begin{equation}
x_t^i = \begin{cases}
\text{[MASK]} & \text{si } M^i < t \\
x_0^i & \text{si } M^i \geq t
\end{cases}
\label{eq:masking_rule}
\end{equation}
$$

El parámetro $t$ actúa como el factor de ruido: $t \rightarrow 0$ significa secuencia limpia; $t \rightarrow 1$ significa secuencia casi totalmente enmascarada.

##### Función de Pérdidas

Durante el entrenamiento, el modelo aprende a predecir el *token* original $x_0$ dadas las entradas enmascaradas $x_t$. La función de pérdidas es una pérdida de *cross-entropy* que solo se aplica a las posiciones que fueron enmascaradas, e incluye un factor de suavizado $\frac{1}{t}$:

$$
\begin{equation}
\mathcal{L} = - \frac{1}{t \cdot L} \sum^L_{i=1} {\mathbf{1}[x_t^i = \text{[MASK]}] \log p_\theta(x^i_0|x_t)} 
\label{eq:loss}
\end{equation}
$$

Este factor $\frac{1}{t}$ es clave: penaliza menos un fallo cuando la secuencia está casi totalmente enmascarada ($t \approx 1$) que cuando solo quedan unos pocos *tokens* por refinar ($t \approx 0$).

![Esquema de la predicción de tokens a partir de una secuencia enmascarada](assets/posts/2025-09-16-llada/predictor.png){: width="600" height="600" }

#### Proceso Reverse (Generación): Desenmascaramiento y Refinamiento

La generación es el proceso inverso y gradual en $T$ pasos, vamos desde el estado completamente enmascarado ($t=1$) hasta el texto limpio ($t=0$).

En lugar de predecir el ruido, el modelo $p_\theta$ predice directamente la distribución de probabilidad de los *tokens* originales para **todas** las posiciones enmascaradas, y lo hace **simultáneamente**.

$$
\begin{equation}
p_\theta(x_{t-1}|x_t) = \text{Predict}(x_t, t) \rightarrow \hat{x}_t
\hat{x}_0 = \text{arg max} p_\theta(x_0 | x_t)
\label{eq:reverse_predict}
\end{equation}
$$

Este paso de **generación paralela** es lo que contrasta fundamentalmente con la naturaleza secuencial del *decoding* autoregresivo.

##### Estrategias de Re-enmascaramiento

Una vez que la red da sus predicciones $\hat{x}_t$, necesitamos decidir qué *tokens* "fijar" y cuáles mantener enmascarados para refinamiento en el siguiente paso. Dos estrategias que se presentan en el paper son:
1.  **Selección Probabilística:** Volver a enmascarar tokens con una probabilidad decreciente en cada paso $ t $, estableciendo como fijos aquellos que no hayan sido enmascarados.
2.  **Enmascaramiento por Confianza:** Un método más inteligente es volver a enmascarar de forma probabilística, pero dando prioridad de refinamiento a aquellos *tokens* donde el modelo ha mostrado **menor confianza** (distribución de probabilidad más uniforme).

##### Eficiencia de Generación

Aquí es donde reside una potencial ventaja frente a los modelos AR: si bien estos requieren $N$ pasadas por la red para $N$ *tokens*, LLaDA solo requiere $T$ pasadas. En escalas donde la longitud de la secuencia es muy grande ($N \gg T$), la **velocidad de generación de LLaDA es superior** gracias a su **naturaleza paralela** de predicción por bloques.

Aunque hay que tener en cuenta que los modelos de diffusion también se beneficián de mayores valores de $ T $ para poder refinar mejor la muestra generada.

### Generación Condicional (*Fine-Tuning*)

La estructura de LLaDA permite una extensión sencilla a la generación condicional, necesaria para tareas como el *in-context learning* o el seguimiento de instrucciones.

Simplemente modelamos el problema como una generación condicionada por un *prompt* $p_0$, donde enmascaramos solo la región destinada a la respuesta generada por el LLM: $p_\theta (x_0 | p_0, x_t)$.

![Esquema de la generación condicional con un prompt de entrada](assets/posts/2025-09-16-llada/prompt.png){: width="600" height="600" }

### Arquitectura y Restricciones
Dado que LLaDA basa su generación en máscaras y requiere atender a toda la secuencia a la vez, debe implementarse con **Transformers no causales** (similares a BERT), a diferencia de los modelos autoregresivos (AR).

Esta arquitectura conlleva una restricción fundamental: la imposibilidad de aplicar optimizaciones de inferencia como el **KV caching**, una limitación que los modelos AR sí aprovechan. 



## Resultados de LLaDA


### La Ventaja Clave: Escalabilidad y Data Efficiency

En el *paper* original, los autores crearon dos modelos *Transformer* básicos de 1 y 8 billones de parámetros para ejecutar las pruebas. Si bien el *paper* presenta un análisis detallado, el resultado más revelador en mi opinión es el relacionado con la **escalabilidad**.

![Escalabilidad de LLaDA en función del cómputo](assets/posts/2025-09-16-llada/scalability.png){: width="800" height="800" }

La figura muestra que la **escalabilidad de LLaDA parece superar a la de los modelos AR** al aumentar la cantidad de cómputo invertido en entrenamiento (incrementando las épocas dado un *dataset* fijo).

Esto conecta con lo descrito en el reciente artículo publicado *[Diffusion Language Models are Super Data Learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac#23bd8f03a86680699f7ad6fa98caa3d2)*, dónde podemos ver los siguientes resultados al comparar modelos de difusión y modelos autorregresivos:

![Eficiencia de datos: Diffusion vs AR](assets/posts/2025-09-16-llada/diffusion_vs_ar_grid.jpg){: width="600" height="600" }

Como se observa, para la misma cantidad de muestras, los modelos de difusión son capaces de **seguir extrayendo información de los datos durante más épocas**, mientras que los modelos AR caen en *overfitting*. Esto se debe a que la difusión reutiliza la misma muestra bajo diferentes niveles de enmascaramiento, ofreciendo distintos gradientes y actuando como un *built-in regularization hack*.



## Mi Implementación Propia

Mis experiencias anteriores con difusión fueron tan frustrantes que el enfoque de LLaDA me impulsó a probar el método y crear una implementación *custom* para una prueba de concepto personal.

Para ello, utilicé el modelo **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (*Transformer* no causal, 66M parámetros), y el dataset: **[tinyStories](https://huggingface.co/datasets/roneneldan/TinyStories)** (historias sencillas generadas por GPT-3.5/4).

El resultado, para un modelo tan pequeño y un entrenamiento tan rápido, fue bastante bueno. Aquí una muestra de su funcionamiento generando una nueva historia mediante el proceso de *desenmascaramiento* iterativo:

![Demo de generación con LLaDA en TinyStories](assets/posts/2025-09-16-llada/generation.gif){: width="800" height="800" }

Personalmente, teniendo en cuenta mis experiencias previas, lo considero un gran éxito. Si quieres ver el código y experimentar, te dejo por aquí el [repositorio de GitHub](https://github.com/javiersgjavi/diffusion-llm/tree/main).

---

## Conclusiones: La Recompensa de Cuestionar los Paradigmas

Personalmente, creo que este trabajo representa un punto de inflexión en cómo abordamos la generación de texto con modelos de difusión.

La lección más importante que extraigo es que la **simplicidad conceptual supera a la complejidad técnica**. Mientras que yo me perdía trabajando con *embeddings* continuos y el **problema de las regiones inválidas**, LLaDA trabaja directamente con *tokens* discretos, resolviendo el problema y haciendo el modelo más interpretable y fácil de depurar.

LLaDA no es una solución mágica. El proceso de generación requiere $T$ predicciones secuenciales. Si este número $T$ no es significativamente menor que la longitud de la secuencia a generar ($N$), las ventajas en eficiencia son discutibles, sobre todo porque no puede aprovechar el *KV caching* de los modelos AR.

Sin embargo, la diferencia es conceptual: implementando LLaDA aprendí que la solución más elegante es a menudo la que se atreve a cuestionar las suposiciones fundamentales. Yo asumía que el espacio continuo era la única forma; LLaDA me demostró que estaba equivocado, adaptando la difusión al molde natural del texto discreto.

LLaDA no es solo un avance técnico, es una invitación a cuestionar los paradigmas establecidos en NLP. Te animo a experimentar y a cuestionar otras suposiciones. La próxima gran innovación podría estar esperando a que alguien se atreva a preguntar "¿por qué no?".

---

## Referencias

### Papers Fundamentales

- [LLaDA: Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992) - Paper original que introduce el concepto de masked diffusion para texto
- [Reflected Diffusion Models](https://arxiv.org/pdf/2304.04740) - Análisis detallado del problema de las regiones inválidas en difusión
- [TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings](https://arxiv.org/pdf/2402.19097) - Enfoque de difusión en espacios latentes continuos
- [Latent Diffusion for Language Generation](https://arxiv.org/pdf/2212.09462) - Uno de los primeros intentos de aplicar difusión a texto
- [Continuous diffusion for categorical data](https://arxiv.org/pdf/2211.15089) - Adaptación de difusión a datos discretos
- [Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/pdf/2107.03006) - Propuesta temprana de difusión discreta

### Recursos Adicionales

- [Diffusion Language Models are Super Data Learners](https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac#23bd8f03a86680699f7ad6fa98caa3d2) - Análisis sobre la eficiencia de aprendizaje
- [Yann LeCun: Why LLMs are Doomed](https://www.youtube.com/watch?v=ETZfkkv6V7Y) - Crítica a las limitaciones de modelos autorregresivos
- [Gemini Diffusion](https://deepmind.google/models/gemini-diffusion/) - Descripción del enfoque de difusión de Google DeepMind
- [Mi implementación de LLaDA](https://github.com/javiersgjavi/diffusion-llm/tree/main) - Repositorio con código de ejemplo


{% include comments.html %}