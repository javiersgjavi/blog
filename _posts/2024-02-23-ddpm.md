---
title: Los modelos de difusión (DDPM)
date: 2024-02-23 00:02:30 +/-1200
categories: [Explicacion]
tags: [difusion]     ## TAG names should always be lowercase
math: true
image:
    path: assets/posts/2024-02-23-ddpm/reverse-diffusion.png
    alt: Modelo de difusión
---



En este primer post intentaré explicar todo lo que he aprendido investigando sobre los modelos de difusión. En principio nos centraremos en el primer artículo original, en el cual se explicaba el DDPM (Denoising Diffusion Probabilistic Model) original, y cuya cita se puede encontrar en el final de este post. Recomiendo echarle un vistazo también al paper original ya que es muy interasante, e intentar entenderlo apoyado en este blog o los otros recursos que dejo al final, ¡que probablemente sean incluso más útiles!

> Si lo que buscas es comprender de manera superficial, o echar un primer vistazo a los modelos de difusión, quizás esta no es la mejor fuente para empezar. Este post contiene lo que a mi me hubiera gustado leer cuando ya conocía más o menos su funcionamiento, pero quería comprender perfectamente como funcionaba y sus bases matemáticas. ¡Si este es tu caso, igualmente te recomiendo echar un vistazo a los recursos que dejo al final!
{: .prompt-warning }

## Resumen inicial

Los modelos de difusión es un tipo de modelo generativo, es decir, se entrena a una red para que aprenda la distribución de unos datos, y pueda generar nuevas muestras pertenecientes a la distribución. Se basa principalmente en un cambio en el algoritmo de entrenamiento. Esto sería la explicación básica y general de lo que es un DDPM.


### Qué no es un modelo de difusión

Sin embargo, más que explicar qué es un modelo de difusión, me parece interesante comenzar definiendo lo que __NO__ es. 

Los DDPM no son:

- Una nueva arquitectura de modelo.
- Un nuevo tipo de modelo.
- Un nuevo tipo de capa neuronal.

Como he dicho justo antes, los modelos de difusión son solo un cambio en el algoritmo de entrenamiento y generación, nada más. Esto es similar a los Generative Adversarial Networks (GANs), ya que en esencia son dos modelos de red neuronales normales, pero que se modifica su algoritmo de entrenamiento para hacer uso de una función de pérdidas adversarial para ajustar sus respectivos parámetros.

### Ventajas y desventajas

![generative_models](assets/posts/2024-02-23-ddpm/generative_models.png){: width="400" height="400" }
_Los 3 pilares de los modelos generativos y sus respectivos puntos fuertos_

Básicamente, un modelo de difusion es un uno de los tres pilares de las técnicas de difusión que hay hoy en día. Y se suele decir que cada pilar es bueno en 2 puntos, pero muy malo en los otros.

Los modelos de difusión tienen las siguientes ventajas:

1. Generan muestras nuevas de mucha calidad.
2. Pueden aprender múltiples distribuciones a la vez.

Pero además, sobre las GANs tienen la ventaja de:

3. Convergencia mucho más estable al no tener dos modelos compitiendo entre sí
4. No sufren de model collapse

Sin embargo, sus desventajas principal es la lentitud de generación. 

En el paper original de los Denoising Diffusion Imputation Models (DDIM), que ya los dejamós para otro momento, se dice que generar 50k imágenes de 32x32 en una tarjeta gráfica Nvidia 2080 Ti tarda menos de un minuto cuando se usan GANs, pero para los modelos de difusión se necista cerca de 1000 horas.

También hay que decir que se empieza a decir que la regla de los 3 pilares se empieza a romper poco a poco, pues ya hay cada vez más trabajos intentando mejorar la velocidad de genearciónd e los modelos de difusion. Las propias DDIM son un ejemplo de ello.

## Funcionamiento general

Un modelo de difusión consiste en ir destruyendo unos datos de partida para corromper su distribución inicial, hasta convertir los datos en puro ruido, es decir, con una distribución normal con media cero y desviación 1, es decir: $\mathcal{N}(0, 1)$.

Para ir corrompiendo estos datos, se va añadiendo máscaras de ruido poco a poco durante una serie de pasos, lo cual se puede definir en forma de cadena de Markov con $T$ pasos. El punto inicial será $x_0$ que es nuestra muestra original, y $x_t$ el final de la cadena, en este punto  es cuando $x_t \sim \mathcal{N}(0, 1)$.

![markov](assets/posts/2024-02-23-ddpm/reverse-diffusion.png){: width="600" height="600" }
_Cadena de Markov_

Para controlar que durante la cadena de Markov la distribución converja adecuadamente a $\mathcal{N}(0, 1)$, se controla la variación que se le aplica con cada máscara de ruido, y esta variación vendrá definida por un parámetro $\beta$. Este parámetro irá variando poco a poco hasta llegar a obtener $x_t \sim \mathcal{N}(0, 1)$, por lo que para cada paso de la cadena tendremos nuestro correspondiente $\beta_t$.

Se suele decir que por lo tanto $\beta_t$ está controlada por el Scheduler, el cual lo podeis imaginar como un objeto de python que defina de alguna forma una lista con la evolución de los valores de $\beta_t$ para cada $t$, y al cual le pedimos que nos de este valor utilizando ```scheduler.get_beta(t)```. Hay distintas técnicas que puede utilizar el Scheduler para definir estos valores, pero el paper original definió el Scheduler lineal.

![image_corruption](assets/posts/2024-02-23-ddpm/schedulers_image.png){: width="600" height="600" }
_Pasamos de la distribución inicial a una máscara de ruido_

![evolucion_beta](assets/posts/2024-02-23-ddpm/evolution_alpha.png){: width="400" height="400" }
_Como el scheduler modifica la media de la distribución a lo largo de la cadena. En el primer caso se presentó el lineal, el del coseno fue presentado más tarde_

Sabiendo esto, básicamente nuestro objetivo va a ser entrenar a un modelo para predecir que máscara de ruido se le añadió en el paso actual. De esta forma, si le restamos la máscara de ruido predicho, deberíamos de ser capaz de recuperar el estado de la muestra en el paso anterior. Si lo hacemos interativamente, deberíamos de ser capaz de utilizar nuestro modelo para predecir las máscaras de ruido de cada paso y poder pasar de $x_t$ a $x_0$.

![generative_models](assets/posts/2024-02-23-ddpm/sketch_loop.png){: width="600" height="600" }
_Bucle para recorrer la cadena al revés y generar una muestra nueva. Se explicará con más detalle más adelante_


## Componentes principales de los modelos de difusión

Dicho esto, para entender bien los modelos de diffusión, hay 5 elementos claves que tenemos que entender:

1. El Scheduler $ab$
2. El forward process $q(x_t$ l $x_{t-1})$
3. La posterior de forward process $q(x_{t-1}$ l $x_t, x_0)$
4. El bakward/reverse process $p_{\theta}(x_{t-1}$ l $x_t)$
5. La función de pérdidas

### Scheduler

Como hemos dicho anteriormente, la cantidad de ruido que vamos a añadir en cada paso de la cadena de Markov no va a ser constante. Está definido de una forma que asegure que cuando llegemos al final de la cadena obtengamos algo como $x_t \sim \mathcal{N}(0, 1)$. Para ello, no podemos añadir una cantidad constante de ruido, si no la variación de la distribución explotaría con la suma de este ruido, es por esto que tenemos que ir escalandolo en cada paso. El factor de escalado en cada paso lo controla el Scheduler con la definición de $\beta_t$.

Básicamente, al principio de cada paso, generaremos una matriz de ruido de la siguiente forma $\epsilon \sim \mathcal{N}(0, 1)$, y luego lo vamos a escalar usando $\beta_t$. Esto lo explicaré en más detalle en acontinuación, pero es importante que hasta este punto se haya comprendido qué es el Scheduler, qué es $\beta_t$, y que la escala de ruido que añadimos en cada paso no es constante.

### Forward/Diffusion process $q(x_t|x_{t-1})$

Okay, estamos diciendo que vamos a ir corrompiendo las muestras iniciales a lo largo de una cadena de Markov en la cual vamos a ir añadiendo ruido progresivamente, ¿pero cómo se hace esto?

Para eso, el paper original nos da la Fórmula \ref{eq:foward} para pasar de cualquier $t-1$ al siguiente paso de la cadena, es decir, $t$:

$$
\begin{equation}
  q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)
  \label{eq:foward}
\end{equation}
$$

No teneis que preocuparos si os resulta rara la Fórmula \ref{eq:foward}, simplemente está aplicando una normal con $\mu = \sqrt{1-\beta_t}x_{t-1}$ y $\sigma^2=\beta_t I$. De hecho, yo personalmente prefiero una versión más explicita de la misma fórmula que presenta el artículo. Para ello, primero definimos la fórmula de la normal:

$$
\begin{equation}
  \mathcal{N}(\mu, \sigma^2) = \mu + \sigma ·\epsilon
  \label{eq:normal}
\end{equation}
$$

$$
\epsilon \sim \mathcal{N}(0,1)
$$


Conociendo definición de la normal en de la Fórmula \ref{eq:normal}, podemos reescribir la Fórmula \ref{eq:foward} de esta forma, la cual personalmente encuentro más agradable:

$$
\begin{equation}
  q(x_t|x_{t-1}) = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon
  \label{eq:foward_explicit}
\end{equation}
$$

  
Si conocemos esta fórmula, para obtener cualquier t, simplemente la aplicamos iterativamente desde $t=0$ tal y como se ve en la siguiente fórmula:

$$
\begin{equation}
  q(x_{1:T}|x_0):=\prod^T_{t=1}{q(x_t|x_{t-1})}
\end{equation}
$$

Sin embargo esto no es para nada práctico, porque esto supondría que durante el entrenamiento, cuando pongamos a prueba a nuestra red condistintos valores de $t$, vamos a tener que recorrer toda la cadena hasta llegar a ese valor. Eso es una locura computacionalmente. Por este motivo, se hacen los siguientes ajustes para conseguir una fórmula que nos permita saltar directamente desde $x_0$ a $x_t$. Primero vamos a definir las siguientes nuevas variables:

$$
\alpha_t = 1 - \beta_t 
$$

$$
\overline{\alpha_t} = \prod^t_{s=1}\alpha_t
$$

De esta forma, $\overline{\alpha_t}$ de alguna forma contine la información sobre la acumulación de todas las $\beta$ anteriores de la cadena, por lo que podemos reformular la Fórmula \ref{eq:foward_explicit} de la siguiente manera:

$$
\begin{equation}
q(x_t|x_0) = \sqrt{\overline{\alpha_t}}x_0 + \sqrt{1-\overline{\alpha_t}}\epsilon
\label{eq:foward_xoxt}
\end{equation}
$$

Y ahora si que sí, con la Fórmula \ref{eq:foward_xoxt} ya tenemos un mecanismo para pasar de cualquier muestra original $x_0$ obtener su versión $x_t$ corrompida hasta llegar al paso $t$ de la cadena de Markov con un único paso. De aquí en adelante, cuando hablemos del Forward process, siempre estaremos hablando de $q(x_t$ l $x_0)$ con la definición de la Fórmula \ref{eq:foward_xoxt}.

### Posterior de forward process $q(x_{t-1}|x_t, x_0)$

Vale, ya sabemos como destruir una muestra hasta llegar al paso que nos dé la gana de la cadena, pero lo importante es ir hacia atrás. Tenemos que encontrar una forma de retroceder por la cadena de markov.

$$
\begin{equation}
q(x_{t-1}|x_t, x_0) = \frac{q(x_{t-1}|x_{t}, x_0)q(x_{t}|x_0)}{q(x_{t-1}| x_0)} 
\label{eq:posterior_bayes}
\end{equation}
$$

Para conseguir esto, mediante una aplicación del teorema de Bayes (si te da curiosidad verlo en más detalle, lo muestro en la Fórmula \ref{eq:posterior_bayes}), podemos obtener este "posterior", el cual nos define el paso previo de la cadena condicionado por el paso actual y la muestra original.

$$
\begin{equation}
q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1};\tilde{\mu}_t(x_t,x_0),\tilde{\beta}_tI)
\label{eq:posterior}
\end{equation}
$$

$$
\begin{equation}
\tilde{\mu}_t(x_t,x_0) := \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t}x_0 + \frac{\sqrt{\alpha_{t}}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t 
\label{eq:mu_tilde}
\end{equation}
$$

$$
\begin{equation}
\tilde{\beta}_t:= \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha_t}}\beta_t
\label{eq:beta_tilde}
\end{equation}
$$

Sin embargo, es un poco incómodo depender en las Fórmulas \ref{eq:posterior} y \ref{eq:mu_tilde} de $x_0$. Además, posteriormente, cuando estemos entrenando la red, tampoco vamos a tener $x_0$, así que como que incomoda un poco ese término ahí, pero no pasa nada, podemos quitarlo con un truquito. Si tenenemos en cuenta que $q(x_t$ l $x_0) = x_t$ podemos despejarnos $x_0$ de la Fórmula \ref{eq:foward_xoxt}, y obtenemos lo siguiente:

$$
\begin{equation}
x_0 = \frac{1}{\sqrt{\overline{\alpha}_t}}(x_t - \frac{1-\alpha}{\sqrt{1-\overline{\alpha}_t}}\epsilon)
\label{eq:x_0_despejado}
\end{equation}
$$

Y si aplicamos la Fórmula \ref{eq:x_0_despejado} a la Fórmula \ref{eq:mu_tilde} se nos queda lo siguiente:

$$
\begin{equation}
\tilde{\mu}(x_t,x_0) = \frac{1}{\sqrt{\alpha}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}}_t}\epsilon)
\label{eq:mu_tilde_despejada}
\end{equation}
$$

Con esto ya tenemos casi todo hecho, tenemos que realizar una última limpieza en la Fórmula \ref{eq:posterior}, para ello vamos a hacer lo siguiente:

1. Aplicar la Fórmula \ref{eq:normal} que define la normal
2. Aplicar la definición de $\tilde{\mu}(x_t,x_0)$ que hemos obtenido en la Fórmula \ref{eq:mu_tilde_despejada}
3. Realizar una pequeña sustitución para limpiar un poco más, ya que sabemos que $x_t = q(x_{t-1}$ l $x_t, x_0)$

Y con todo esto, ya obtenemos lo siguiente:

$$
\begin{equation}
x_{t-1} = \frac{1}{\sqrt{\alpha}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}}}\epsilon_t) + \sqrt{\beta_t}\epsilon
\label{eq:posterior_clean}
\end{equation}
$$

Con la Fórmula \ref{eq:posterior_clean} ya podemos retroceder a lo largo de la cadena de Markov. Pero date cuenta de un detallito: hay dos $\epsilon$ distintas. En este caso yo he llamado a una $\epsilon_t$ y otra $\epsilon$ simplemente para diferenciarlas, pero lo importante es comprender qué es cada una:

1. $\epsilon$ es el ruido que se añade al aplicar la matriz de ruido para cumplir las caracteristicas de la normal que se le aplica para volver al paso previo

2. $\epsilon_t$ es lo realmente importante aquí. Este es el valor de todo el ruido acumulado que se le ha aplicado a $x_0$ para llegar a $x_t$

### Backwards process $p_{\theta}(x_{t-1}|x_t)$

Ahora que tenemos claro como movernos por la cadena tanto hacia adelante como hacia atrás, lo que nos interesa es enseñar a nuestro modelo a retroceder. Es por ello, que con lo que queremos es que $p_{\theta}(x_{t-1}$l $x_t)$ aprenda a como replicar $q(x_{t-1}$ l $x_t, x_0)$.

En un principio, el proceso hacia atrás se define de la siguiente forma:

$$
\begin{equation}
p(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t),\Sigma_\theta(x_t, t))
\label{eq:backward_full}
\end{equation}
$$

Básicamente, similar a la Fórmula \ref{eq:posterior}, tenemos que ser capaces de predecir la distribución de la muestra en el paso previo. Para ello, deberíamos de ser capaces de predecir con algún modelo (o dos distintos) tanto la normal $\mu_\theta(x_t, t)$ como varianza $\Sigma_\theta(x_t, t)$. Sin embargo, los creadores del artículo original vieron que se obtenía mejores resultados sin predecir $\Sigma_\theta(x_t, t)$, y total, como al fin y al cabo conocemos eso porque viene establecido por el Scheduler, pues no lo ahorramos (aunque artículos posteriores si que demuestran beneficios en predecir $\Sigma_\theta(x_t, t)$). De esta forma se nos queda así la fórmula anterior: 

$$
\begin{equation}
p(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \beta_t I)
\label{eq:backward_mu}
\end{equation}
$$

Entonces, llegados a este punto, la media que tenemos que predecir es la siguiente: 

$$
\begin{equation}
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1 - \overline{\alpha_t}}}\epsilon_\theta(x_t, t))
\label{eq:mu_prediction}
\end{equation}
$$

Y ahí ya podemos ver que realmente, el único parámetro a predecir es $\epsilon_\theta(x_t, t)$, que es básicamente, el ruido que se le ha añadido durante toda la cadena hasta llegar a $t$, y por lo tanto, como se ha deformado la media hasta llegar a hasta este punto de la cadena. Si precedimos este ruido, podemos volver a escalarlo para el paso actual con el término que le múltiplica $\frac{\beta_t}{\sqrt{1 - \overline{\alpha_t}}}$ ir eliminandolo progresivamente y así ir volviendo hacia atrás.

Con todo esto que hemos dicho, si reconstruimos la Fórmula \ref{eq:backward_mu} con lo desglosado en la Fórmula \ref{eq:mu_prediction} y con la definición de la normal de la Fórmula \ref{eq:normal}, y tenemos en cuenta que $x_{t-1} = p(x_{t-1}$ l $x_t)$ llegamos a la ecuación final para retroceder a lo largo de la cadena:

$$
\begin{equation}
x_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1 - \overline{\alpha_t}}}\epsilon_\theta(x_t, t)) + \beta_t\epsilon
\label{eq:backwards_final}
\end{equation}
$$

Hay que tener que una vez más, vuelve a aparecer dos $\epsilon$ distintos, los volvemos a definir:

1. $\epsilon$ es el ruido que se le aplica según la definición de la normal
2. $\epsilon_\theta(x_t, t)$ es el valor de ruido que se ha predicho para calcular la media, y que se ha definido un par de párrafos antes.

#### Definición alternativa con $q(x_{t-1}|x_t, \hat{x}_0)$

A pesar de que la Fórmula \ref{eq:backwards_final} es la más ampliamente utilizada y que más se explica al hablar de los modelos de difusión, en ocasiones os vais a encontrar que realmente se aplica $ q(x_{t-1} $ l $ x_t, \hat{x}_0) $ para retroceder a lo largo de la cadena.

Realmente $ q(x_{t-1}\text{l}x_t, \hat{x}_0) $ y $ p(x_{t-1}\text{l}x_t) $ son prácticamente lo mismo, sólo que de forma más explicita, y por lo tanto no habiendo despejado $x_0$, como se demuestra en el proceso de obtener la Fórmula \ref{eq:posterior_clean} desde la Fórmula \ref{eq:posterior}.

Cuando os encontreis con este caso en una fórmula, no os preocupeis demasiado, lo único que se hace es generar $\hat{x}_0$ con la siguiente fórmula:

$$
\begin{equation}
\hat{x}_0 = \frac{1}{\sqrt{\overline{\alpha_t}}}(x_t - \sqrt{1-\overline{\alpha_t}}\epsilon_\theta(x_t, t))
\label{eq:x_pred}
\end{equation}
$$

####  Consideraciones importantes

##### La imporantancia de t

Es importante darse cuenta de la importancia de pasarle como $t$ como parámetro de entrada a los modelos. Esto puede verse ya que durante esta sección hemos visto que todos los parámetros a predecir tienen este valor como un input para el modelo.

Esto es así porque como hemos dicho anteriormente, el ruido que se añade no es constante, por lo que el modelo debe de conocer el paso actual para poder estimar correctamente el ruido.

##### Dificultad en mantener $x_0 \in [-1, 1]$

Un problema de los modelos de difusión es que al ir eliminando ruido para retrocer a lo largo de la cadena de Markov, les cuesta mantener los valores de las muestras en el rango de [-1, 1]. Hay distintas fórmas de evitar esto, una muy común es limitar el valor máximo que pueden alcanzar estos, pero no es muy eficaz; otro punto forma puede ser normalizar la muestra de nuevo.

##### En el último paso no añadimos ruido

Como hemos visto en la Fórmula \ref{eq:backwards_final} cuando retrocedemos por la cadena añadimos ruido para completar las definiciones de las normales, pero cuando lleguemos al último paso de la cadena, al dar el paso de $x_1$ a $x_0$, no se añade ningún ruido. Tiene sentido si lo piensas: queremos recuperar la muestra original, no queremos añadirle ningún ruido adicional al final

### Función de pérdidas

Nos queda la última pata de todo esto: la función de pérdidas. Hasta ahora hemos visto como se puede recorrer la cadena de Markov, y como nuestro modelo puede retroceder por ella, pero ¿cómo podemos enseñarle al modelo a ejecutar esta tarea?

La función de pérdidas debería de ser simplemente la log-likehood de $p_\theta(x_0)$, sin embargo, esto nos requeriría recorrer toda la cadena para cada batch de entrenamiento, lo cual es muy costoso, o como definen en el artículo, intractable.

Por eso, vamos a entrenar haciendo uso del Variational Lower Bound, que es una función que siempre es al menos mayor que negative log-likehood, por lo tanto, si minimizamos esta función, tenemos que estar obligatoriamente mejorando la negative log-likehood.

![elbo](assets/posts/2024-02-23-ddpm/elbo.png){: width="600" height="600" }
_Ejemplo de Likehood y Variational Lower Bound. Está última siempre estará por debajo de Likehood, y por lo tanto, mejorarla, acaba mejorando el Likehood._

Básicamente, eso lo podeis ver con la siguiente fórmula:

$$
\begin{equation}
\mathbb{E}[\underbrace{-\log{p_\theta(x_0)}}_{NLL}]\le\mathbb{E}_q[-\underbrace{\log p_\theta(x_0|x_1)}_{L_0}  + \sum_{t>1}\underbrace{D_{KL}(q(x_{t-1}|x_t,x_0) || p_\theta(x_{t-1}|x_t))}_{L_{t-1}} +\underbrace{D_{KL}(q(x_T|x_0) || p(x_T))}_{L_T}]
%\label{eq:elbo}
\end{equation}
$$

La ecuación de arriba se que puede parecer complicada, pero vamos a ir desglosandola poco a poco:

1. $NLL$: Es la negative log-likehood, se puede ver que siempre va a ser al igual o menor que lo que hay a la derecha de la inecuación.

De aquí en adelante, vamos a estar comparando la similitud entre las distribuciones reales $q$, y las que aprenda nuestro modelo $p_\theta$, esto lo vamos a hacer comparándolas mediante la KL divergencia. Entonces:

2. $L_0$ sería la parte que mide la likehood del primer paso de la cadena
3. $L_{t-1}$ sería la likehood de los pasos intermedios de la cadena
4. $L_{T}$ es el último paso de la cadena. Como aquí realmente ya lo que tenemos son dos matrices de ruido con una distribución $\sim\mathcal{N}(0,1)$, y que está controlado por los parámetros del Scheduler, podemos eliminar esto de la ecuación.

Sin embargo, no teneis que tener miedo de esta pedazo de fórmula, porque ya sabemos que realmente lo único que va a determinar que el proceso de reconstrucción esté bien aproximado es una correcta estimación de $\epsilon_\theta(x_t, t)$. Por eso mismo, mediante una limpieza de la Fórmula \ref{eq:elbo}, llegamos a lo siguiente:

$$
\begin{equation}
\mathcal{L} = \mathbb{E}_{t, x_o, \epsilon} [||\epsilon - \epsilon_\theta(x_t, t)||^2]
\label{eq:loss}
\end{equation}
$$

Es básicamente el MSE entre el error real y el predicho, algo sin duda mucho más manejable.

## Algoritmos

Okay, ya conocemos las báses matemáticas detrás de los algoritmos de difusión, hora de analizar los algoritmos. En este caso, a diferencia de las GAN, es cierto que se van a diferenciar bastante el de entrenamiento con el de generación.

### Entrenamiento

![training_algorithm](assets/posts/2024-02-23-ddpm/training.png){: width="500" height="500" }
_Algoritmo de entrenamiento_

Básicamente el funcionamiento es el siguiente:

1. Seleccionamos un batch de entrenamiento
2. Generamos aleatoriamente un valor de $t$ distinto para cada muestra
3. Generamos los $x_t$ según el $t$ que le haya tocado a cada uno y guardamos la matriz de ruido de cada una.
4. Usamos nuestro modelo para predecir la matriz de ruido en función de $x_t$ y $t$
5. Calculamos el mse entre el ruido real y el predicho y retropropagamos el error.

### Generación

![sampling_algorithm](assets/posts/2024-02-23-ddpm/sampling.png){: width="500" height="500" }
_Algoritmo de generación de muestras_

Se ve que es un poco diferente al anterior, pero básicamente lo que tenemos que hacer es lo siguiente:

1. Generar un batch con tantas matrices de ruido aleatorio como muestras queramos generar. Estas serán nuestras $x_t$
2. Realizar la predicción del ruido que tienen con nuestro modelo utilizando $x_t$ y $t$.
3. Reconstruir $x_{t-1}$ con el ruido predicho.
4. Repetir hasta haber recorrido toda la cadena.

De esta forma, apartir de un ruido totalmente aleatorio, nuestro modelo ha generado unas muestras totalmente nuevas.

## UNET: la primera arquitecura utilizada

![unet](assets/posts/2024-02-23-ddpm/unet_lateral.png){: width="600" height="600" }
_Representación de Unet_

La primera arquitectura que se utilizó con los modelos de diffusión fue la Unet, ya que es una arquitectura muy famosa en el campo de la visión por computador, y este primer artículo lo que generaba eran imágenes.

Sin embargo, hay una particularidad cuando se implementa la Unet para los modelos de difusión, tal y como se puede ver en esta imagen que representaría una vista desde arriba de la arquitectura:

![unet_arriba](assets/posts/2024-02-23-ddpm/unet_arriba.png){: width="600" height="600" }
_Representación de Unet vista desde arriba_

Se puede ver que el valor de la t no se da sólo como entrada en la red, si no que se códifica con múltiples señales senoidales mediante el positional encoding (algo básico en la arquitectura de los transformers), y se va recordando constantemente en el interior del modelo.

### Consideraciones especiales

Personalmente, hay dos cosas muy importantes que he podido verificar experimentalmente cuando se implementan modelos de difusión:

1. Tal y como se ve en la Figura (), hay que estar recordando continuamente los valores de t por dentro de la imagen.

2. Pero también hay que aplicar _skip_ _connections_ y recordar lo que se había extraído en capas anteriores, tal y como se puede ver en las líneas discontinuas grises.

Sin estas dos cosas, probablemente los modelos de difusión no puedan converger adecuadamente.

## ¿Qué viene después?

Vale, creo que si has llegado hasta aquí se puede considerar que ya tienes una idea bastante profunda sobre los modelos de difusión, o al menos sobre lo que exponía el paper original. Pero es muy conocido que en los útlimos años la investigación en IA está creciendo de forma exponencial, por lo que es normal que desde que se presentaron en 2020, se hayan hecho muchos avances. 

Por aquí te dejo un resumen de algunos de los más básicos e importantes, y espero que en un futuro pueda cubrir algunos de ellos.

### Improving diffusion

Este artículo de OpenAI explora el uso del Cosine Scheduler que destruye más gradualmente la información de la muestra original a lo largo de la cadena de Markov. Además, exponen las ventajas de predecir $\Sigma_\theta(x_t,t)$, cosa que el artículo original directamente descartó.

### CFG: Classifier-free guidance

Este artículo propone una forma de enseñarle a los modelos de difusión la distribuciónd e varias clases a la vez, y a cómo pedirles que generen muestras de una clase concreta, todo esto mediante el uso de información condicional.

También enseñan que esto puede mejorar la calidad de las muestras que se generan.

### DDIM: Denosing Diffusion Implicit Models

Este artículo ya lo hemos comentado brevemente anteriormente, pero es una variación sobre los modelos de difusión que permite que, entrenandolos exactamente igual, poder saltarse pasos de la cadena de Markov a la hora de generar muestras. Esto los hace irremediablemente más rápidos.

## Fuentes recomendadas

- [Artículo original de DDPM](https://arxiv.org/pdf/2006.11239.pdf)

En Youtube teneis estos videos que me fueron cruciales para que pudiera comprenderlo todo:

Este primer video es en una maravilla, en 30 minutos te explica todos los conceptos matemáticos de la diffusion, probablemente la mejor fuente que he encontrado para entenderlo todo.

{% include embed/youtube.html id='HoKDTa5jHvg' %}


Este es una continuación del video anterior, en este implementa un modelo de difusión en Pytorch desde 0, una vez más, recomendadísimo.

{% include embed/youtube.html id='TBCRlnwJtZU' %}

Este video también está muy bien, ya que lee el paper y te va explicando todo paso a paso.

{% include embed/youtube.html id='y7J6sSO1k50' %}


{% include comments.html %}