---
title: Los modelos de difusión (DDPM)
date: 2024-02-23 00:02:30 +/-1200
categories: [Explicacion]
tags: [difusion]     # TAG names should always be lowercase
math: true
image:
    path: assets/posts/2024-02-23-ddpm/reverse-diffusion.png
    alt: Modelo de difusión
---


# Primeras aclaraciones

En este primer post intentaré explicar todo lo que he aprendido investigando sobre los modelos de difusión. En principio nos centraremos en el primer artículo original, en el cual se explicaba el DDPM (Denoising Diffusion Probabilistic Model) original, y cuya cita se puede encontrar en el final de este post. Recomiendo echarle un vistazo también al paper original ya que es muy interasante, e intentar entenderlo apoyado en este blog o los otros recursos que dejo al final, ¡que probablemente sean incluso más útiles!

## Resumen inicial

Los modelos de difusión es un tipo de modelo generativo, es decir, se entrena a una red para que aprenda la distribución de unos datos, y pueda generar nuevas muestras pertenecientes a la distribución. Se basa principalmente en un cambio en el algoritmo de entrenamiento. Esto sería la explicación básica y general de lo que es un DDPM.


## Qué no es un modelo de difusión

Sin embargo, más que explicar qué es un modelo de difusión, me parece interesante comenzar definiendo lo que __NO__ es. 

Los DDPM no son:

- Una nueva arquitectura de modelo.
- Un nuevo tipo de modelo.
- Un nuevo tipo de capa neuronal.

Como he dicho justo antes, los modelos de difusión son solo un cambio en el algoritmo de entrenamiento y generación, nada más. Esto es similar a los Generative Adversarial Networks (GANs), ya que en esencia son dos modelos de red neuronales normales, pero que se modifica su algoritmo de entrenamiento para hacer uso de una función de pérdidas adversarial para ajustar sus respectivos parámetros.

# Funcionamiento general

Un modelo de difusión consiste en ir destruyendo unos datos de partida para 
# Componentes principales de los modelos de difusión

Para comenzar 

## Forward process $q(x_t|x_{t-1})$

## Posterior $q(x_{t-1}|x_t, x_0)$

## Backwards process $p_{\theta}(x_{t-1}|x_t)$

## Scheduler

## Función de pérdidas

## ¿Por qué estamos preciendo $\orange{\epsilon_t}$?

# Algoritmos

## Entrenamiento

## Generación

# UNET: la primera arquitecura utilizada

# ¿Qué viene después?

## Improving diffusion

## CFG: Classifier-free guidance

## Añadir información condicional

## DDIM: Denosing Diffusion Implicit Models

# Fuentes recomendadas

- [Artículo original de DDPM](https://arxiv.org/pdf/2006.11239.pdf)

En Youtube teneis estos videos que me fueron cruciales para que pudiera comprenderlo todo:

Este primer video es en una maravilla, en 30 minutos te explica todos los conceptos matemáticos de la diffusion, probablemente la mejor fuente que he encontrado para entenderlo todo.

{% include embed/youtube.html id='HoKDTa5jHvg' %}


Este es una continuación del video anterior, en este implementa un modelo de difusión en Pytorch desde 0, una vez más, recomendadísimo.

{% include embed/youtube.html id='TBCRlnwJtZU' %}

- Este video también está muy bien, ya que lee el paper y te va explicando todo paso a paso.

{% include embed/youtube.html id='y7J6sSO1k50' %}


{% include comments.html %}