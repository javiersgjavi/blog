---
title: Los modelos de difusión (DDPM)
date: 2024-02-23 00:02:30 +/-1200
categories: [Explicacion]
tags: [difusion]     # TAG names should always be lowercase
math: true
image:
    path: assets/posts/2024-02-23-ddpm/reverse-diffusion.png
    alt: Modelo de difusión
---



En este primer post intentaré explicar todo lo que he aprendido investigando sobre los modelos de difusión. En principio nos centraremos en el primer artículo original, en el cual se explicaba el DDPM (Denoising Diffusion Probabilistic Model) original, y cuya cita se puede encontrar en el final de este post. Recomiendo echarle un vistazo también al paper original ya que es muy interasante, e intentar entenderlo apoyado en este blog o los otros recursos que dejo al final, ¡que probablemente sean incluso más útiles!

# Resumen inicial

Los modelos de difusión es un tipo de modelo generativo, es decir, se entrena a una red para que aprenda la distribución de unos datos, y pueda generar nuevas muestras pertenecientes a la distribución. Se basa principalmente en un cambio en el algoritmo de entrenamiento. Esto sería la explicación básica y general de lo que es un DDPM.


## Qué no es un modelo de difusión

Sin embargo, más que explicar qué es un modelo de difusión, me parece interesante comenzar definiendo lo que __NO__ es. 

Los DDPM no son:

- Una nueva arquitectura de modelo.
- Un nuevo tipo de modelo.
- Un nuevo tipo de capa neuronal.

Como he dicho justo antes, los modelos de difusión son solo un cambio en el algoritmo de entrenamiento y generación, nada más. Esto es similar a los Generative Adversarial Networks (GANs), ya que en esencia son dos modelos de red neuronales normales, pero que se modifica su algoritmo de entrenamiento para hacer uso de una función de pérdidas adversarial para ajustar sus respectivos parámetros.

# Funcionamiento general

Un modelo de difusión consiste en ir destruyendo unos datos de partida para corromper su distribución inicial, hasta convertir los datos en puro ruido, es decir, con una distribución normal con media cero y desviación 1, es decir: $\mathcal{N}(0, 1)$.

Para ir corrompiendo estos datos, se va añadiendo máscaras de ruido poco a poco durante una serie de pasos, lo cual se puede definir en forma de cadena de Markov con $T$ pasos. El punto inicial será $x_0$ que es nuestra muestra original, y $x_t$ el final de la cadena, en este punto  es cuando $x_t \sim \mathcal{N}(0, 1)$.

Para controlar que durante la cadena de Markov la distribución converja adecuadamente a $\mathcal{N}(0, 1)$, se controla la variación que se le aplica con cada máscara de ruido, y esta variación vendrá definida por un parámetro $\beta$. Este parámetro irá variando poco a poco hasta llegar a obtener $x_t \sim \mathcal{N}(0, 1)$, por lo que para cada paso de la cadena tendremos nuestro correspondiente $\beta_t$.

Se suele decir que por lo tanto $\beta_t$ está controlada por el Scheduler, el cual lo podeis imaginar como un objeto de python que defina de alguna forma una lista con la evolución de los valores de $\beta_t$ para cada $t$, y al cual le pedimos que nos de este valor utilizando ```scheduler.get_beta(t)```. Hay distintas técnicas que puede utilizar el Scheduler para definir estos valores, pero el paper original definió el Scheduler lineal.

> Tengo que añadir aquí las imágenes de como se corrompe una imagen por la cadena, y como cambia la evolución del beta
{: .prompt-danger }

Sabiendo esto, básicamente nuestro objetivo va a ser entrenar a un modelo para predecir que máscara de ruido se le añadió en el paso actual. De esta forma, si le restamos la máscara de ruido predicho, deberíamos de ser capaz de recuperar el estado de la muestra en el paso anterior. Si lo hacemos interativamente, deberíamos de ser capaz de utilizar nuestro modelo para predecir las máscaras de ruido de cada paso y poder pasar de $x_t$ a $x_0$.

> Imagen mia del bucle del modelo de la presentación
{: .prompt-danger }

# Componentes principales de los modelos de difusión

Dicho esto, para entender bien los modelos de diffusión, hay 5 elementos claves que tenemos que entender:

- El Scheduler
- El forward process $q(x_t|x_{t-1})$
- La posterior $q(x_{t-1}|x_t, x_0)$
- El bakward/reverse process $p_{\theta}(x_{t-1}|x_t)$
- La función de pérdidas

## Scheduler

Como hemos dicho anteriormente, la cantidad de ruido que vamos a añadir en cada paso de la cadena de Markov no va a ser constante. Está definido de una forma que asegure que cuando llegemos al final de la cadena obtengamos algo como $x_t \sim \mathcal{N}(0, 1)$. Para ello, no podemos añadir una cantidad constante de ruido, si no la variación de la distribución explotaría con la suma de este ruido, es por esto que tenemos que ir escalandolo en cada paso. El factor de escalado en cada paso lo controla el Scheduler con la definición de $\beta_t$.

Básicamente, al principio de cada paso, generaremos una matriz de ruido de la siguiente forma $\epsilon \sim \mathcal{N}(0, 1)$, y luego lo vamos a escalar usando $\beta_t$. Esto lo explicaré en más detalle en acontinuación, pero es importante que hasta este punto se haya comprendido qué es el Scheduler, qué es $\beta_t$, y que la escala de ruido que añadimos en cada paso no es constante.

## Forward/Diffusion process $q(x_t|x_{t-1})$

Okay, estamos diciendo que vamos a ir corrompiendo las muestras iniciales a lo largo de una cadena de Markov en la cual vamos a ir añadiendo ruido progresivamente, ¿pero cómo se hace esto?

Para eso, el paper original nos da la Fórmula \ref{eq:foward} para pasar de cualquier $t-1$ al siguiente paso de la cadena, es decir, $t$:

$$
\begin{equation}
  q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)
  \label{eq:foward}
\end{equation}
$$

No teneis que preocuparos si os resulta rara la Fórmula \ref{eq:foward}, simplemente está aplicando una normal con $\mu = \sqrt{1-\beta_t}x_{t-1}$ y $\sigma^2=\beta_t I$. De hecho, yo personalmente prefiero una versión más explicita de la misma fórmula que presenta el artículo. Para ello, primero definimos la fórmula de la normal:

$$
\begin{equation}
  \mathcal{N}(\mu, \sigma^2) = \mu + \sigma ·\epsilon \\ 
  \epsilon \sim \mathcal{N}(0,1)
  \label{eq:normal}
\end{equation}
$$
$$
\epsilon \sim \mathcal{N}(0,1)
$$


Conociendo definición de la normal en de la Fórmula \ref{eq:normal}, podemos reescribir la Fórmula \ref{eq:foward} de esta forma, la cual personalmente encuentro más agradable:

$$
\begin{equation}
  q(x_t|x_{t-1}) = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon
  \label{eq:foward_explicit}
\end{equation}
$$

  
Si conocemos esta fórmula, para obtener cualquier t, simplemente la aplicamos iterativamente desde $t=0$ tal y como se ve en la siguiente fórmula:

$$
\begin{equation}
  q(x_{1:T}|x_0):=\prod^T_{t=1}{q(x_t|x_{t-1})}
\end{equation}
$$

Sin embargo esto no es para nada práctico, porque esto supondría que durante el entrenamiento, cuando pongamos a prueba a nuestra red condistintos valores de $t$, vamos a tener que recorrer toda la cadena hasta llegar a ese valor. Eso es una locura computacionalmente. Por este motivo, se hacen los siguientes ajustes para conseguir una fórmula que nos permita saltar directamente desde $x_0$ a $x_t$. Primero vamos a definir las siguientes nuevas variables:

$$
\alpha_t = 1 - \beta_t 
$$
$$
\overline{\alpha_t} = \prod^t_{s=1}\alpha_t
$$

De esta forma, $\overline{\alpha_t}$ de alguna forma contine la información sobre la acumulación de todas las $\beta$ anteriores de la cadena, por lo que podemos reformular la Fórmula \ref{eq:foward_explicit} de la siguiente manera:

$$
\begin{equation}
q(x_t|x_0) = \sqrt{\overline{\alpha_t}}x_0 + \sqrt{1-\overline{\alpha_t}}\epsilon
\label{eq:foward_xoxt}
\end{equation}
$$

Y ahora si que sí, con la Fórmula \ref{foward_xoxt} ya tenemos un mecanismo para pasar de cualquier muestra original $x_0$ obtener su versión $x_t$ corrompida hasta llegar al paso $t$ de la cadena de Markov con un único paso. De aquí en adelante, cuando hablemos del Forward process, siempre estaremos hablando de $q(x_t|x_0)$ con la definición de la Fórmula \ref{foward_xoxt}.

## Posterior $q(x_{t-1}|x_t, x_0)$

Vale, ya sabemos como destruir una muestra hasta llegar al paso que nos dé la gana de la cadena, pero lo importante es ir hacia atrás. Tenemos que encontrar una forma de retroceder por la cadena de markov.

Para conseguir esto, mediante una aplicación del teorema de Bayes, podemos obtener este "posterior", el cual nos define el paso previo de la cadena condicionado por el paso actual y la muestra original.

$$
\begin{equation}
q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1};\tilde{\mu}_t(x_t,x_0),\tilde{\beta}_tI)
\label{eq:posterior}
\end{equation}
$$

$$
\begin{equation}
\tilde{\mu}_t(x_t,x_0) := \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t}x_0 + \frac{\sqrt{\alpha_{t}}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t 
\label{eq:mu_tilde}
\end{equation}
$$

$$
\begin{equation}
\tilde{\beta}_t:= \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha_t}}\beta_t
\label{eq:beta_tilde}
\end{equation}
$$

Sin embargo, es un poco incómodo depender en las Fórmulas \ref{posterior} y \ref{mu_tilde} de $x_0$. Además, posteriormente, cuando estemos entrenando la red, tampoco vamos a tener $x_0$, así que como que incomoda un poco ese término ahí, pero no pasa nada, podemos quitarlo con un truquito. Si tenenemos en cuenta que $q(x_t|x_0) = x_t$ podemos despejarnos $x_0$ de la Fórmula \ref{foward_xoxt}, y obtenemos lo siguiente:

$$
\begin{equation}
x_0 = \frac{1}{\sqrt{\overline{\alpha}_t}}(x_t - \frac{1-\alpha}{\sqrt{1-\overline{\alpha}_t}}\epsilon)
\label{eq:x_0_despejado}
\end{equation}
$$

Y si aplicamos la Fórmula \ref{x_0_despejado} a la Fórmula \ref{mu_tilde} se nos queda lo siguiente:

$$
\begin{equation}
\tilde{\mu}(x_t,x_0) = \frac{1}{\sqrt{\alpha}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}}_t}\epsilon)
\label{eq:mu_tilde_despejada}
\end{equation}
$$

Con esto ya tenemos casi todo hecho, tenemos que realizar una última limpieza en la Fórmula \ref{posterior}, para ello vamos a hacer lo siguiente:

1. Aplicar la Fórmula \ref{normal} que define la normal
2. Aplicar la definición de $\tilde{\mu}(x_t,x_0)$ que hemos obtenido en la Fórmula \ref{mu_tilde_despejada}
3. Realizar una pequeña sustitución para limpiar un poco más, ya que sabemos que $x_t = q(x_{t-1}|x_t, x_0)$

Y con todo esto, ya obtenemos lo siguiente:

$$
\begin{equation}
x_{t-1} = \frac{1}{\sqrt{\alpha}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}}}\epsilon_t) + \sqrt{\beta_t}\epsilon
\label{eq:posterior_clean}
\end{equation}
$$

Con la Fórmula \ref{posterior_clean} ya podemos retroceder a lo largo de la cadena de Markov. Pero date cuenta de un detallito: hay dos $\epsilon$ distintas. En este caso yo he llamado a una $\epsilon_t$ y otra $\epsilon$ simplemente para diferenciarlas, pero lo importante es comprender qué es cada una:

1. $\epsilon$ es el ruido que se añade al aplicar la matriz de ruido 

## Backwards process $p_{\theta}(x_{t-1}|x_t)$

## Función de pérdidas

## ¿Por qué estamos preciendo $\orange{\epsilon_t}$?

# Algoritmos

## Entrenamiento

## Generación

# UNET: la primera arquitecura utilizada

# ¿Qué viene después?

## Improving diffusion

## CFG: Classifier-free guidance

## Añadir información condicional

## DDIM: Denosing Diffusion Implicit Models

# Fuentes recomendadas

- [Artículo original de DDPM](https://arxiv.org/pdf/2006.11239.pdf)

En Youtube teneis estos videos que me fueron cruciales para que pudiera comprenderlo todo:

Este primer video es en una maravilla, en 30 minutos te explica todos los conceptos matemáticos de la diffusion, probablemente la mejor fuente que he encontrado para entenderlo todo.

{% include embed/youtube.html id='HoKDTa5jHvg' %}


Este es una continuación del video anterior, en este implementa un modelo de difusión en Pytorch desde 0, una vez más, recomendadísimo.

{% include embed/youtube.html id='TBCRlnwJtZU' %}

- Este video también está muy bien, ya que lee el paper y te va explicando todo paso a paso.

{% include embed/youtube.html id='y7J6sSO1k50' %}


{% include comments.html %}